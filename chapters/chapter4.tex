%!TEX root = ../main.tex

\graphicspath{{./figures/chapter4/}}

\chapter{Spatial Feature Engineering}
\label{ch:chapter4}

\minitoc
\newpage

In this chapter, I present and discuss different approaches to derive quantitative profiles describing \ac{RNA} localization patterns.
By combining the outputs from detection and segmentation modules (Chapters~\ref{ch:chapter2} and~\ref{ch:chapter3} respectively), each cell is characterized by a point cloud (\ac{RNA}), a set of landmarks (e.g.~the nucleus or other cellular compartments) and potentially by some pre-digested subpatterns (such as \ac{RNA} clusters).
The challenge is then to derive a vector that is suitable for downstream analysis, namely quantification, supervised and unsupervised analysis.

In the first part, I describe the preparation of the input data, starting from segmentation and detection results obtained by the methods implemented in \emph{bigfish.multitask}.
In the second part, I manually select or design a set of localization features.
These features are implemented in \emph{bigfish.classification} and described in~\cite{Imbert_fq_2022}.
In the third part, I present a different approach to compute spatial features.
By training a point cloud model on a pretext task, I build a feature extractor that learns a relevant embedding~\cite{pointfish_2022}.
This approach is described in the paper:

\begin{center}
	\color{green}
	A. Imbert, F. Mueller, et al. (2022), \textit{PointFISH: learning point cloud representations for RNA localization patterns}, in 2022 European Conference on Computer Vision (ECCV 2022) Workshop on BioImage Computing.
\end{center}

\section{From images to coordinates}
\label{sec:image_coordinates}

The segmentation and detection methods presented in chapters~\ref{ch:chapter2} and~\ref{ch:chapter3} provide us with a list of detected objects (e.g.~\ac{RNA} and clusters) and segmentation masks (e.g.~nucleus and cytoplasmic region), as illustrated in Figure~\ref{fig:cell_extracted_0}.
Here, the coordinates of the detected objects and segmentation masks are extracted and form the input data for which we will seek a quantitative description, amenable to machine learning.

We recall from the previous chapter that \emph{bigfish} proposes 2D and 3D detection, but only 2D segmentation.
This is not a conceptual limitation, but rather a pragmatic choice in our projects.
Also, any external methods for detection and segmentation could be used, as long as output formats are compatible with \emph{bigfish}.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter4/cell_extracted_0}
    \caption[Coordinate representation of a cell]{Contrasted original image with segmented boundaries (\textit{left}) and coordinate representation (\textit{right}).
	Plot build with \emph{bigfish}}
    \label{fig:cell_extracted_0}
\end{figure}

\subsection{Gathering all information at the single cell level}
\label{subsec:coordinate_representation}

To extract and summarize \ac{FISH} results at the single cell level, the only requirement is a segmentation mask of the cell, that allows us to associate each detected \ac{RNA} and other objects to one cell.
Segmentation of additional compartments is optional, but greatly improves the amount of relevant information on \ac{RNA} localization assigned to each cell.

\subsubsection{Labeling detected objects according to their localization in the cell}

The nuclei segmentation masks allows us to identify the intranuclear \ac{RNA}s.
Technically, this amounts to assigning a region label to each of the detected \ac{RNA}s or clusters.
In general, the more subcellular compartments are available, the more detailed the description of \ac{RNA} localization can become in the following.

This compartmentalization might also change the interpretation we are giving to detected clusters.
For instance, a cluster inside a nucleus might be interpreted as transcription site, while a cluster in the cytoplasm can have various biological interpretations (e.g.~P-bodies, translation factories, stress granules).

In \emph{bigfish} this labeling of detected \ac{RNA}s and clusters can be achieved with a few lines:\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# discriminate foci and transcription sites
spots_no_ts, foci, ts = multistack.remove_transcription_site(
    rna=spots,
    clusters=clusters,
    nuc_mask=nuc_label,
    ndim=3)
\end{lstlisting}
\end{minipage}

\subsubsection{The advantage of a coordinate representation}

In Figure~\ref{fig:cell_extracted_0} we see a cropped \ac{smFISH} image on the left, with cell and nuclear membranes in red and blue respectively.
On the right, these membranes are illustrated in black and blue respectively.
In addition, we see \ac{RNA} spots (in red), \ac{RNA} clusters (in orange, with the estimated number of \ac{RNA} clustered) and transcription sites (in green).

With such \emph{extraction} we lose pixel-wise information like intensity values or image texture.
We also rely on detection and segmentation performances to return meaningful coordinates.
Nonetheless, coordinate representation is a sparse and more natural representation for \ac{mRNA} localization pattern classification: the coordinates - rather than the fluorescence images themselves - represent the actual measurement I want to make, when I perform smFISH experiments.
Finally, in contrast to raw images, coordinates of detected objects allow a much more convincing data integration: the number of potential batch effects and biases is greatly reduced.\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# extract cell-level results
fov_results = multistack.extract_cell(
    cell_label=cell_label,
    ndim=3,
    nuc_label=nuc_label,
    rna_coord=spots_no_ts,
    others_coord={"foci": foci, "transcription_site": ts},
    image=image_contrasted,
    others_image={"dapi": nuc_mip, "smfish": smfish_mip})
\end{lstlisting}
\end{minipage}

\subsubsection{Filtering}

We can filter the data at the cellular and object level.

At the cellular level, we might want to ensure that only one nucleus is assigned to each cell, and we can remove cells at the border of the \ac{FoV}.
Their segmentation is incomplete and might bias final results.
Third, extracted cells can be filtered out according to the number of detected objects (especially the number of \ac{RNA}s).
By censoring empty cells, we remove potential outliers, detection or segmentation failures and therefore help a subsequent statistical analysis.

We can also filter out individual objects according to their localization in the cell.
A user might want to exclude a detected object if it locates within a segmented surface.
For example, some studies require to remove transcription sites before further analysis~\cite{CHOUAIB_2020}, or on the contrary focus on quantification of transcription sites.

\subsection{Statistical description}
\label{subsec:statistical_description}

At this stage, standard and useful statistics for every cell can already be computed.
In particular, we can measure cell and nucleus areas, but also \ac{RNA} distribution, inside and outside nucleus.
From cluster coordinates, we can estimate cluster size, as well as proportion of clustered \ac{RNA}s.
\ac{RNA} proportion in specific cellular compartments are also noteworthy.
Such indicators are already relevant to quantify or validate meaningful biological properties.
For example, a recent study~\cite{cochard_rna_2022} uses \emph{bigfish} to estimate \ac{RNA} recruitment in bioengineered condensates (segmented from a \ac{GFP} channel).\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# compute cell-level statistics
df = multistack.summarize_extraction_results(fov_results, ndim=3)
\end{lstlisting}
\end{minipage}

\section{Hand-crafted localization features}
\label{sec:hand_features}

Here I present hand-crafted features I have implemented for the analysis of \ac{RNA} localization patterns.
In section~\ref{subsec:related_work_hand_features}, I present related work.
We will see that there is already a large number of features available in the literature.
In section~\ref{subsec:expert_features}, I present the features I have chosen for our studies.

\subsection{Related work}
\label{subsec:related_work_hand_features}

\subsubsection{Feature engineering with bioimages}

In bioimage analysis, in particular for High Content Screening, there is an abundant literature on feature families that have been used for cell and whole image classification.
They can be roughly categorized into shape features and texture features.
Examples include Haralick features~\cite{Haralick1973}, statistical geometric features~\cite{Walker1996}, local binary patterns~\cite{ahonen_2006}, moment based features~\cite{Reeve1992} and morphological granulometries~\cite{Serra1983}, to name a few.

These features have been used to classify cells according to their protein localization patterns~\cite{boland_automated_1998, Glory2007}, or their phenotypes~\cite{Wang2008, Jones2009, Walter2010} or classify directly full images with entire cell populations~\cite{Uhlmann2016}.

Importantly, such feature families are readily available in standard open-source software for computational phenotyping, such as CellProfiler~\cite{Carpenter2006, Jones2008, mcquin_cellprofiler_2018}, CellCognition~\cite{held_cellcognition_2010}, Mahotas~\cite{mahotas_2013} and ilastik~\cite{berg_ilastik_2019}.

Most of these hand-crafted features come from the computer vision literature: they are pretty generic with limited biological interpretability.
Furthermore, they are designed to quantitatively describe object shapes and textures, rather than directly representing spatial distributions of biomolecules.
Of note, this also holds for protein localization screens~\cite{Glory2007, ouyang_analysis_2019}.
This is due to the fact that with traditional microscopy, individual proteins cannot be resolved.
In addition, proteins can be abundant and often concentrate on specific structures.
As a consequence, the protein signal cannot be decomposed into single points, and it is therefore logical to describe their spatial distribution by texture features.
A notable exception is provided by super-resolution microscopy, where protein distributions are indeed modeled as point clouds~\cite{Levet2019}.

In conclusion, there is a need for features describing spatial distributions of \ac{RNA} molecules inside the cell, implemented in user-friendly open-source tools.

\subsubsection{Feature design to describe RNA localization}

Hand-crafted features to classify \ac{RNA} localization patterns were already developed in previous studies~\cite{battich_image-based_2013,samacoits_computational_2018}.
Generally such features are inspired by literature on spatial statistics~\cite{ripley2005spatial} and adapted for fluorescence microscopy images~\cite{lagache_statistical_2015,stueland_rdi_2019}.
Furthermore, several packages implement modules to perform \ac{smFISH} analysis and compute these hand-crafted features~\cite{mueller_fish-quant_2013,savulescu_dypfish_2019,mah_bento_2022}.

In~\cite{battich_image-based_2013}, authors designed 18 features ''that reflect the relative localization of each spot in a single cell, with respect to both the cell and other spots''.
The authors compute per-transcript features, then exploit their per-cell mean and standard deviation to identify subcellular localization patterns.
%In a subsequent work~\cite{stoeger_computer_2015}, the workflow is completed and extended by integration of segmentation and detection algorithms.

Two recent Python libraries propose algorithms for the quantification of \ac{FISH} experiments.
DypFISH~\cite{savulescu_dypfish_2019} was developed to analyze \ac{RNA} and protein (co-)localization.
It includes features to investigate clustering and \ac{MTOC}-related patterns.
Across different imaging acquisitions of cells with a architecture constraints, they propose techniques to study \ac{mRNA}-protein spatial distribution~\cite{savulescu_interrogating_2021}.
Bento~\cite{mah_bento_2022} proposes modules to analyze images generated with \ac{SeqFISH} and \ac{MERFISH}.
They adapt feature extraction pipeline to highly multiplexed spatial transcriptomics data.

The features I present in this chapter extends previous work in our groups, implemented in MATLAB~\cite{mueller_fish-quant_2013, samacoits_computational_2018}, where more than 20 features were proposed to analyze mRNA localization patterns.
Compared to previous feature sets~\cite{battich_image-based_2013}, they led to better performance in \ac{RNA} pattern classification, as shown by simulations.
A first set of features includes distance features between \ac{RNA} spots and cell or nucleus.
A second set of features involved the Ripley K-function~\cite{ripley2005spatial}:

\begin{equation}
	{\displaystyle K(r) = \frac{1}{n} \sum_{i = 1}^{n} \frac{N_i(r)}{\lambda}}
\end{equation}

\noindent
With $N_i(r)$ the number of \ac{RNA}s in a circle of radius $r$ centered on the $i^{th}$ \ac{RNA} and $\lambda$ the total density of \ac{RNA}s in the cell.
It quantifies the aggregation or dispersion of mRNAs.
Several features are designed from these values, exploiting their maximum or their correlation with the radius $r$.
However, Ripley features are sensitive to boundary effects (\ac{RNA}s close to a membrane have a limited neighborhood) and require to be correctly normalized.
A third set of features is based on assessing \ac{RNA} density measured in subcellular regions, defined by morphological operators.
Lastly, dispersion and polarization indices are implemented.

\subsection{Hand-crafted features to describe RNA localization patterns}
\label{subsec:expert_features}

Here, I present the features that I have implemented in \emph{bigfish.classification}.
Many of these features were designed for our own studies~\cite{CHOUAIB_2020,safieddine_choreography_2021,pichon_kinesin_2021}, but are sufficiently generic to be of general utility.
These features capture more specific information about \ac{RNA} localization, beyond expression levels (\emph{nb\_rna}).

One of the important aspects is the normalization of these features.
Indeed, many features are per definition highly dependent on the expression level or the cellular morphology.
Here, the objective is to define features that characterize preferential localization of \ac{RNA}, independently from expression levels.

\subsubsection{Distance features}

In order to capture preferential localization with respect to the cell membrane, we define \emph{index\_mean\_distance\_cell}:

\begin{equation}
	{\displaystyle \operatorname{index\_mean\_distance\_cell} = \frac{\overline{d_{cell}(x_i, y_i)}}{\lambda_{cell}}}
\end{equation}

\noindent
With $d_{cell}(x_i, y_i)$ the euclidean distance to the cell membrane for the \ac{RNA} $i$ and $\lambda_{cell}$ the expected average distance under uniform \ac{RNA} distribution.
$\lambda_{cell}$ is calculated efficiently as the average value of the 2D distance map from the cell membrane.
The normalization with respect to $\lambda_{cell}$ makes the feature robust with respect to different cell morphologies~\cite{samacoits_computational_2018}: we measure how much we deviate from a uniform distribution, given the cell morphology and the expression level.

Similarly, we compute the normalized average distance of \ac{RNA}s to the nucleus: \emph{index\_mean\_distance\_nuc}.
Alternative computation with the median function is available for these two features too.

In the current version, segmentations are provided in 2D (and consequently, the distances are also defined in 2D).

\subsubsection{Compartment density features}

The idea of this feature family is to partition the cellular region into several subregions and to evaluate the \ac{RNA} density in these regions.
This corresponds to spatial histograms of \ac{RNA} distribution.
These features come in two variants:

\begin{enumerate}
	\setlength\itemsep{0.1em}
	\item \emph{propotion} is the proportion of RNAs inside the region with respect to the total number of RNAs in the cell.
	\item \emph{index} is the ratio between measured and expected RNAs inside the region.
\end{enumerate}

The choice of the partition then conditions the kind of patterns that will can be differentiated with the feature family.
First, as already indicated above, we defined as a coarse partition the nuclear and cytoplasmic region and calculate the proportion of \ac{RNA}s within the nucleus (\emph{proportion\_rna\_in\_nuc}).

Second, we defined regions according to distance intervals to the nucleus and the cell membrane respectively.
Region $R_i$ is thus defined as:

\begin{equation}
    R_i = \{ x \, \, | \, \, d_{compartment}(x) \in  [\tau_i, \tau_{i+1}] \}
\end{equation}

\noindent
where $x$ is a point in the image plane, and \emph{compartment} can mean the nucleus or cytoplasmic membrane.
We count the number of \ac{RNA} falling in $R_i$ and calculate the proportion and the index. 

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter4/plot_topography}
    \caption[Proportion of mRNAs in several subcellular regions]{Proportion of mRNAs at various distances from cell and nuclear membranes.
    Dataset and annotated patterns from~\cite{CHOUAIB_2020}.
    \textit{Colored areas} represent the 95\% confidence interval}
    \label{fig:features_topography}
\end{figure}

In our studies, we defined regions with respect to their distance to the nucleus, where the interval boundaries were $\tau in \{-500, 500, 1000, 1500, 2000\}$ (in nm) and where $d_{nucleus} < 0$ refers to points inside the nucleus.
Furthermore, we defined regions with respect to their distance to the cytplasmic membranes, using the interval boundaries $\tau in \{0, 5000, 1000, 1500, 2000\}$ (in nm).
In summary, we thus have:

\begin{itemize}
	\setlength\itemsep{0.1em}
    \item \emph{proportion\_rna\_nuc\_radius\_$\tau_i$\_$\tau_{i+1}$}
    \item \emph{index\_rna\_nuc\_radius\_$\tau_i$\_$\tau_{i+1}$}
    \item \emph{proportion\_rna\_cell\_radius\_$\tau_i$\_$\tau_{i+1}$}
    \item \emph{index\_rna\_cell\_radius\_$\tau_i$\_$\tau_{i+1}$}
\end{itemize}

In order to investigate the discriminative power of these features, we visualized the average \ac{RNA} proportion for the different regions in Figure~\ref{fig:features_topography} and the corresponding 95\% confidence interval for different patterns.
These patterns will be introduced in detail in chapter~\ref{ch:chapter5}.
In brief, \emph{intranuclear} refers to an accumulation of \ac{RNA} inside the nucleus, \emph{nuclear edge} to an accumulation in proximity of the nuclear envelope, \emph{perinuclear} to a preferred localization close to the nucleus but less pronounced than \emph{nuclear edge}, \emph{protrusion} to an accumulation in cell extensions, \emph{foci} to \ac{RNA} clusters and \emph{random} to the absence of all the other patterns.
As expected, nuclear edge and perinuclear patterns present a higher proportion of \ac{RNA}s along the nuclear membrane.
On the opposite, cells with a protrusion pattern have a higher \ac{RNA} density along the cell membrane.

Sometimes, it can also make sense to probe for the presence of \ac{RNA}s in specific regions that are not necessarily present in all cells.
In order to quantify the \ac{RNA} presence in protrusions for instance, we build the partition by a morphological top-hat: $f-\gamma_B f$, where $\gamma_B$ is the morphological opening with a structuring element $B$.
This operation extracts the parts of the cytoplasm that cannot contain the structuring element.
Here, we use a disk with radius $300 nm$.

\subsubsection{Dispersion features}

I implemented three features described and tested in a recent paper~\cite{stueland_rdi_2019} to quantify \ac{RNA} polarization and dispersion within the cell.

Polarization index is computed by comparing \ac{RNA} point cloud centroid and cell centroid:

\begin{equation}
	{\displaystyle \operatorname{index\_polarization} = \frac{\sqrt{(x_{rna} - x_{cell})^2 + (y_{rna} - y_{cell})^2}}{Rg_{cell}}}
\end{equation}

\noindent
With $(x_{rna}, y_{rna})$ the coordinates of the \ac{RNA} centroid and $(x_{cell}, y_{cell})$ the coordinates of the cell centroid.
The radius of gyration $Rg_{cell}$ normalizes the index for different cell sizes.
It is defined as the root-mean-squared distance between every cell pixel and the cell centroid.
The higher, the more polarized \ac{RNA}s are.

The dispersion index measures the dispersion of the \ac{RNA} point cloud.
In addition to the extracted coordinates, its computation also implies pixel intensities from the original \ac{smFISH} image:

\begin{equation}
	{\displaystyle \operatorname{index\_dispersion} = \frac{\frac{\sum_{i} d_i^2 I_i}{\sum_{i} I_i}}{\frac{\sum_{j} d_j^2 I_j}{\sum_{j} I_j}}}
\end{equation}

\noindent
With $d_i$ and $d_j$ the euclidean distance of \ac{RNA} $i$ and cell pixel $j$ to the \ac{RNA} centroid respectively, $I_i$ the pixel intensity of \ac{RNA} $i$ and $I_j$ the pixel intensity of cell pixel $j$.
Pixel intensity of transcripts distant from the \ac{RNA} centroid are overweighted.
As the index is normalized considering every pixel $j$ from the cell mask, it tends to 1 when \ac{RNA} point cloud is uniformly distributed.
A diffuse point cloud has a value greater than 1.
On the opposite, if \ac{RNA}s are concentrated anywhere in the cell, index value is lower than 1.

The peripheral distribution index measures how close the \ac{RNA}s localize to the cell periphery (\emph{index\_peripheral\_distribution}).
Its computation is similar to the dispersion index, but the \ac{RNA} centroid is replaced by the nucleus centroid in the equation.
A completely dispersed point cloud still has a value of 1, but it increases if \ac{RNA}s move toward the cell periphery, with a concentrated or diffused pattern.
Again, an aggregation of transcripts around the nucleus centroid (often close to the cell centroid too) decreases index value.

\subsubsection{Centrosomal features}

\begin{wrapfigure}{R}{0.40\textwidth}
	\begin{center}
	\includegraphics[width=\linewidth]{figures/chapter4/centrosomal_features}
	\caption[Centrosomal neighborhood]{Centrosomal RNAs and its neighborhood from~\cite{safieddine_choreography_2021}}
	\label{fig:centrosome_features}
	\end{center}
\end{wrapfigure}

The nuclear and cytoplasmic membranes are major landmarks of the cell, but in more specific screens, it might be interesting to assess the distance distributions with respect to other compartments in the cell, if such compartments are labeled in the experiments.
For instance, I have worked on a screen, where we wanted to study \ac{RNA} localization related to the Microtubule Organizing Centers \ac{MTOC}~\cite{safieddine_choreography_2021}, which I detail in chapter~\ref{ch:chapter5}, where centrosomes were fluorescently labeled.

A first obvious feature is the average (or median) distance between \ac{RNA}s and the closest detected centrosomes (up to two centrosomes can be detected in the cell): \emph{index\_mean\_distance\_centrosome}.
Similarly to the distance features I compute with the cell or nuclear membranes, I compute the expected distance under uniform \ac{RNA} distribution for normalization.

A second set of features consists in delimiting an area around each centrosome to be considered as centrosome's neighborhood.
In my case I manually choose a radius of 2000nm around centrosomes to define such areas, as illustrated in Figure~\ref{fig:centrosome_features}.
I can then compute normalized \ac{RNA} count (\emph{index\_rna\_centrosome}) or \ac{RNA} proportion (\emph{proportion\_rna\_centrosome}) in these regions.

Lastly, I derived a feature from the dispersion index described above.
I define a centrosomal dispersion index to quantify \ac{RNA} dispersion around centrosomes: \emph{index\_centrosome\_dispersion}.
The feature is designed like the dispersion index, but instead of \ac{RNA} centroid, I use the closest centrosome coordinates to compute the euclidean distance.
The lower, the closer \ac{RNA}s localized to the centrosomes.

\subsubsection{Cluster features}

In addition, to the distances to landmarks, it is often useful to also analyze inter-point distances.
In particular, small clusters of \ac{RNA}s are of biological interest.
For this, we decided to rather represent these clusters by detection (see subsection~\ref{subsec:dense_decomposition}) and not by generic inter-point features, such as those based on the Ripley's K-function.
More specifically, number of detected clusters (\emph{nb\_foci}) or \ac{RNA} proportion inside these clusters (\emph{proportion\_rna\_in\_foci}) are relevant features to identify transcripts with a tendency for clustering.\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.classification as classification

# compute features
features, features_names = classification.compute_features(
    cell_mask=cell_mask,  # individual cell mask
	nuc_mask=nuc_mask,  # individual nucleus mask
	ndim=3,
	rna_coord=rna_coord,
    smfish=smfish,
	voxel_size_yx=103,  # in nanometer
    foci_coord=foci_coord,
    compute_distance=True,
    compute_intranuclear=True,
    compute_protrusion=True,
    compute_dispersion=True,
    compute_topography=True,
    compute_foci=True,
    compute_area=True,
    return_names=True)
\end{lstlisting}
\end{minipage}

\section{Learned localization features}
\label{sec:learned_features}

A second approach to analyze \ac{RNA} localization patterns is to learn features.
To this end I designed and trained a deep learning model, PointFISH, on a simulated pretext task~\cite{pointfish_2022}.
I then reused the internal representation learned by the model as a point cloud embedding to discriminate \ac{RNA} localization patterns.
The later was evaluated on an experimental dataset.

\subsection{Related work}
\label{subsec:related_work_learned_features}

\subsubsection{Learning features and embeddings}

A neural network learns representations that can be used for transfer learning.
One advantage is to pre-train relevant representations on a first task with a large and general annotated dataset, before addressing a more difficult or specific task with sometimes a limited dataset available.
Such model can be used as a feature extractor by computing features from one of its intermediate layers.
Computer vision community progressively replaces hand-crafted features~\cite{Lowe_1999,Bay_2006} by deep learning features to analyze images.
The best convolutional neural networks pretrained on large and general classification challenges~\cite{He_2016, Szegedy_2016_CVPR, Tan_2019, Huang_2017_CVPR} are used as backbone or feature extractor for more complex task like face recognition, detection or segmentation.
NLP community follows this trend as well with a heavy use of word embeddings~\cite{Mikolov_2013,Joulin_2016} or the more recent transformer models.
As a last example, with graph computation, node2vec~\cite{Grover_2016} learns ''task-independent representations'' for nodes in networks.

Such embeddings have also the advantage to be a continuous and numerical representation.
This is especially useful when dealing with a non-structured data like text or graph.
With spage2vec~\cite{Partel_2021}, authors learn a low dimensional embedding of local spatial gene expression (expressed as graphs).
Eventually, they identify meaningful gene expression signatures by computing this embedding for tissue datasets.

\subsubsection{Convolutional features}

Because I analyze smFISH images, a first intuition would be to build a convolutional neural network to directly classify localization patterns from the fluorescent image.
This approach is actually already in place for protein localization.
Unlike RNA, proteins are usually studied with fluorescent protein, like \ac{GFP} markers, and are in general impossible to resolve individually (except by using super-resolution microscopy, a technique that is not used in HCS today).
In this case, they appear as a gradient of intensity in the fluorescent image and thus protein localization has often been approached like texture classification problem.
First studies~\cite{boland_automated_1998} for example compute a set of feature from the microscopy image before training a classifier.
With recent successes of deep learning, protein localization is now tackled with convolutional neural network, but still framed as a texture classification problem.
After crowdsourcing annotations for the Human Protein Atlas dataset~\cite{Uhlen_2015} (through a video game), researchers trained a machine learning model (Loc-CAT) from hand-crafted features to predict subcellular localization patterns of proteins~\cite{sullivan_deep_2018}.
In a second time, they organized an online challenge~\cite{ouyang_analysis_2019} where a majority of top-ranking solutions were based on convolutional neural networks.
For protein localization the shift from hand-crafted features to convolutional features is significant and allows more accurate and robust pipelines.

A recent perspective paper~\cite{Savulescu_2021} fosters the use of deep learning models for RNA localization analysis.
Today, such analysis can be performed with fluorescent images or RNA sequencing.
Authors emphasize the recent successes and flexibility of neural nets with both types of input, and therefore the possibility to design a multimodal pipeline.
However, smFISH images have clear spots that can be individually resolved and easily detected.
Therefore, a texture classification approach seems suboptimal to address \ac{RNA} localization pattern recognition.

% references CNN for biological images
% Moen, E. et al. Deep learning for cellular image analysis. Nat. Methods https://doi.org/10.1038/s41592-019-0403-1 (2019).
% Godinez, W. J., Hossain, I., Lazic, S. E., Davies, J. W. & Zhang, X. A multi-scale convolutional neural network for phenotyping high-content cellular images. Bioinforma. Oxf. Engl. 33, 2010–2019 (2017).
% Hofmarcher, M., Rumetshofer, E., Clevert, D.-A., Hochreiter, S. & Klambauer, G. accurate prediction of biological assays with high-throughput microscopy images and convolutional networks. J. Chem. Inf. Model. 59, 1163–1171 (2019).
% Kraus, O. Z., Ba, J. L. & Frey, B. J. Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics 32, i52–i59 (2016).

\subsubsection{Point cloud models}

I postulate that learning to classify RNA localization patterns directly from detected spots coordinates is the most efficient approach.
A point cloud has an unordered and irregular structure.
Projecting the coordinates into images or voxels~\cite{Maturana_2015} transforms the problem as an easier vision challenge, but it comes along with some input degradations.
It dramatically increases the memory needed to process the sample and loses relevant spatial information.
In case of \ac{RNA} point cloud, I have contributed to a study, where we explored this approach by using convolutional neural network on a binary image of the point cloud~\cite{dubois_deep_2019}.
Qualitative analysis of the clustering results indicated that the method does capture relevant signal, but it still cumbersome to first detect individual \ac{RNA}s, to represent them in a binary image, which is then processed with a CNN~\cite{dubois_deep_2019} (a more detailed description of the method is available in appendix~\ref{ch:convolutional_features}).

A recent paper~\cite{khater_caveolae_2019} proposes to train a machine learning pipeline to discriminate caveolae clusters from scaffolds clusters.
These cell structures can be recognized through the detection of caveolin-1 proteins and the shape of the resulting point clouds acquired from \ac{SMLM} techniques.
The authors compare three pipelines to address the problem: a random forest classifier trained on hand-crafted features, a convolutional neural network applied on the point cloud image and more importantly a PointNet fed directly with point cloud.
Albeit legitimate, the PointNet method is less successful for this task than the two others pipelines.

PointNet~\cite{Qi_2017_CVPR} is a seminal work that leads the way for innovative models to address shape classification.
It directly processes point clouds with shared MLPs and a max pooling layer, making the network invariant to input permutation.
However, the pooling step is the only way for the model to share information between close points, which ultimately limits its performance.
Yet, recent research dramatically improves point cloud modelling and especially the capture of local information.

PointNet++~\cite{Qi_2017} learns local geometric structures by recursively applying PointNet to different regions of the point cloud, in a hierarchical manner.
This way, local information can be conveyed through the network more efficiently.
DGCNN~\cite{Wang_2019} proposes a new EdgeConv layer where edge features are computed between a point and its neighbors.
Some models propose to adapt convolutions to point cloud by designing new weighting functions or kernel operations like PointCNN~\cite{Li_2018}, PointConv~\cite{Wu_2019_CVPR} or KPConv~\cite{Thomas_2019_ICCV}.
Another inspiration from the computer vision or NLP literature is the attention-based model.
To this end, PointTransformer~\cite{Zhao_2021_ICCV} proposes an attention layer to be applied to local regions within the point cloud.
Last but not least, PointMLP~\cite{ma2022rethinking} proposes a simple but still efficient network with a pure deep hierarchical MLP architecture.

\subsection{Problem statement}
\label{subsec:problem_statement}

The objective is to train a model, that takes as input directly the point cloud coordinates as an input and compute a continuous vector representation.% suitable for downstream classification tasks.
This representation can then be used for classification of different \ac{RNA} localization patterns.
Such a deep learning model might require a large volume of annotated data to reach satisfying performance.
Manual annotation of \ac{RNA} localization patterns is complex and time consuming.
For rare patterns, it might even be impossible to find cells in sufficient numbers.
For this reason, I turned to simulation to build a large training set, from which I can train a point cloud neural network, providing a feature vector that describes \ac{RNA} spatial distributions.
Finally, I evaluate these learned features on a experimental dataset.

\begin{figure}[]
	\centering
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_10}
		\subcaption{10\% clustered RNA}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_50}
		\subcaption{50\% clustered RNA}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_90}
		\subcaption{90\% clustered RNA}
	\endminipage
	\caption[Simulated foci patterns]{Foci pattern with increasing pattern strength simulated with \emph{simfish}}
	\label{fig:foci_panel}
\end{figure}

\subsubsection{Localization pattern simulations}

To build my simulated dataset, I use methods implemented in \emph{simfish}.
My package exploits a template of 318 real cells to simulate realistic point clouds.
They were originally extracted for FISH-quant v1~\cite{samacoits_computational_2018} to simulate realistic cell and nucleus shapes.
The 3D segmentation masks for the cell and the nucleus were extracted from smFISH experiments against GAPDH, an abundant house-keeping gene localizing in the cytoplasm.
With respect to the previous version~\cite{samacoits_computational_2018}, I made the following improvements:

\begin{itemize}
	\setlength\itemsep{0.1em}
	\item Migration to Python and removal of any proprietary dependencies.
	\item Significant acceleration of the simulation process.
	\item Extension of available localization patterns.
	\item Redefinition of pattern strength to make simulations more consistent.
\end{itemize}

Simulation's outcome includes the cell mask and its membrane coordinates (in 2D), the nucleus mask and its membrane coordinates (in 2D) and the \ac{RNA} coordinates (in 3D).
To match with the rest of the \emph{bigfish} pipeline, I voluntarily return 2D membrane coordinates.

For the simulation, we have to set 3 parameters: (1) the number $n$ of \ac{RNA}s, (2) the pattern strength $p$ referring to the proportion of \ac{RNA}s that localize according to the simulated pattern, where $(1-p) \times n$ \ac{RNA}s are placed uniformly across the cell or cytoplasm and (3) the localization pattern for the $n_{pattern} = p \times n$ \ac{RNA}s. 

\begin{figure}[]
	\centering
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_10}
		\subcaption{10\% perinuclear RNA}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_50}
		\subcaption{50\% perinuclear RNA}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_90}
		\subcaption{90\% perinuclear RNA}
	\endminipage
	\caption[Simulated perinuclear patterns]{Perinuclear pattern with increasing pattern strength simulated with \emph{simfish}, from~\cite{pointfish_2022}}
	\label{fig:perinuclear_panel}
\end{figure}

The 9 patterns currently available are: random, foci, intranuclear, extranuclear, nuclear edge, perinuclear, cell edge, pericellular and protrusion.
Random pattern is the default pattern where \ac{RNA}s are simulated uniformly within the cell.
The foci pattern consists in a random number of \ac{RNA} clusters localizing outside the nucleus.
I draw the expected number of \ac{RNA}s $\lambda$ per cluster from a uniform distribution between 5 and 21 \ac{RNA}s.
The number of clusters itself results from $n_{cluster} = \frac{n_{pattern}}{\lambda}$.
For each cluster, a distinct number of \ac{RNA}s is drawn again with a Poisson distribution of mean $\lambda$.
Clusters are then localized outside the nucleus and remaining \ac{RNA}s uniformly in the cell.
In Figure~\ref{fig:foci_panel} we can observe foci simulations with an increasing pattern strength (from 10\% to 90\% of clustered \ac{RNA}s).

Other patterns are simulated with a similar scheme.
In a first step I generate a probability map to bias the localization of $n_{pattern}$ \ac{RNA}s.
In a second step I complete the point cloud with random \ac{RNA}s until I reach the expected number of transcripts.
Intranuclear pattern has a uniform probability map inside the nucleus and zeros outside.
Extranuclear pattern is the exact opposite, with nonzero probabilities outside the nucleus and zero inside.
Nuclear edge and cell edge have nonzero probabilities along the nuclear and cell membranes.
Similarly, perinuclear and pericellular are patterns where \ac{RNA}s are polarized towards nuclear and cell membranes.
Perinuclear probability map is build from the cell distance map.
The euclidean distance is contrasted by computing its quadratic values.
The same operation is performed to build the pericellular probability map, using the nucleus distance map.
As a result, for pericellular pattern, \ac{RNA}s have a higher probability to localize in regions distant from nucleus.
The protrusion pattern has a uniform probability map within the annotated protrusion regions and zeros everywhere else.
In Figure~\ref{fig:perinuclear_panel} different perinuclear simulations can be observed as an example.
In addition, an overview of every simulated pattern is available in appendix~\ref{sec:appendix_simulations_pattern}.\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import simfish as sim

# load template dataset
path_template_directory = load_extract_template(path_output)

# localization pattern simulation
instance_coord = sim.simulate_localization_pattern(
	path_template_directory,
	n_spots=150,
	pattern="intranuclear",
	proportion_pattern=0.6)
\end{lstlisting}
\end{minipage}

\subsubsection{Simulated dataset}

\begin{wraptable}{R}{0.50\textwidth}
	\centering
	\begin{tabular}{| c | c |}
		\hline
		Pattern & \# of cells \\
		\hline
		Random & 372\\
		Foci & 198\\
		Intranuclear & 73\\
		Nuclear edge & 87\\
		Perinuclear & 64\\
		Protrusion & 83\\
		\hline
	\end{tabular}
	\caption[Annotated experimental dataset for RNA localization patterns]{Annotated experimental dataset}
	\label{table:real_dataset}
\end{wraptable}

With \emph{simfish} I simulate a dataset with 8 different localization patterns: random, foci, intranuclear, extranuclear, nuclear edge, perinuclear, cell edge and pericellular.
I choose these patterns since they represent a diverse panel of localization patterns in different subcellular regions.
I simulate for each pattern 20,000 cells with 50 to 900 \ac{RNA}s per cell, resulting in a full dataset of 160,000 simulated cells.
Random pattern excepted, every simulated pattern has a proportion of \ac{RNA}s with preferential localization ranging from 60\% to 100\%.
With random simulations the pattern strength has no effect.
I split my dataset between train, validation and test, with 60\%, 20\% and 20\% respectively.
In order to avoid data leakage, I make sure that simulations from the same cell template cannot be assigned to different splits.
Finally, point clouds are augmented with random rotation along the up-axis, centered and normalized into the unit sphere.
In order to test how the trained features generalize to unknown localization patterns, I did not simulate \ac{RNA} localization in protrusions, while these localization class is present in the experimental dataset.

\subsubsection{Experimental dataset}

I use the experimental dataset extracted from our study~\cite{CHOUAIB_2020} to validate the feature representation learned on simulated images.
Images are obtained from a \ac{smFISH} study in HeLa cells targeting 27 different genes, then processed with \emph{bigfish}.
After cleaning, it consists of 9710 individual cells, with cropped images and coordinates extracted.
Cells have on average 346 \ac{RNA}s in average and 90\% of them have between 39 and 1307 transcripts.
Furthermore, 810 cells have manually annotated localization patterns, as detailed in Table~\ref{table:real_dataset} and illustrated in Figure~\ref{fig:localization_patterns_racha_features}.
Importantly, these patterns are not mutually exclusive since cells can display several patterns at the same time (for example foci with a perinuclear distribution).
I use these annotations as a ground truth for validation.

\begin{figure}[]
	\centering
	\minipage{0.2\textwidth}
		\includegraphics[width=\linewidth]{figures/introduction/real_coord_foci}
		\subcaption{Foci}
	\endminipage\hfill
	\minipage{0.2\textwidth}
		\includegraphics[width=\linewidth]{figures/introduction/real_coord_intranuclear}
		\subcaption{Intranuclear}
	\endminipage\hfill
	\minipage{0.2\textwidth}
		\includegraphics[width=\linewidth]{figures/introduction/real_coord_nuclear_edge}
		\subcaption{Nuclear edge}
	\endminipage\hfill
	\minipage{0.2\textwidth}
		\includegraphics[width=\linewidth]{figures/introduction/real_coord_perinuclear}
		\subcaption{Perinuclear}
	\endminipage\hfill
	\minipage{0.2\textwidth}
		\includegraphics[width=\linewidth]{figures/introduction/real_coord_protrusion}
		\subcaption{Protrusion}
	\endminipage
	\caption[Coordinate representations for different RNA localization patterns]{RNA localization patterns from~\cite{CHOUAIB_2020}.
	Coordinate representations with RNA spots (\textit{red}), cell membrane (\textit{black}) and nuclear membrane (\textit{blue}).
	Detection and segmentation results are extracted and visualized with \emph{bigfish}}
	\label{fig:localization_patterns_racha_features}
\end{figure}

\subsection{PointFISH}
\label{subsec:pointfish}

\subsubsection{Input preparation}

% old version: 
% Besides the original \ac{RNA} point cloud, I can use an optional second input vector with my model.
% Let $X \in \mathbb{R}^{N \times 3}$ be the original input point cloud with $N$ the number of \ac{RNA}s.
% I define my second input vector as $\tilde{X} \in \mathbb{R}^{N \times d}$ with $d \in \{1, 2, 3, 4, 5\}$.
% The latter is composed of three contextual inputs.
% First I can integrate morphological information by merging \ac{RNA} point cloud with 2D coordinates from the cell and the nucleus membranes.
% Such coordinates are localized to the average height of the \ac{RNA} point cloud (0 if it is centered).
% This morphological input substantially increases the size of the original point cloud, because I subsample 300 nodes from the cell membrane and 100 nodes from the nucleus one.
% I also define an extra boolean vector to indicate the cell nodes and a second one to label the nucleus nodes.
% By construction, each \ac{RNA} in the point cloud has then two \emph{False} values.
% I end up with $X \in \mathbb{R}^{\tilde{N} \times 3}$ (with $\tilde{N} = N + 300 + 100$) and $\tilde{X} \in \{0, 1\}^{\tilde{N} \times 2}$ as inputs.
% Second, I can compute the distance from cell and nucleus for every \ac{RNA} node.
% This adds an extra input $\tilde{X} \in \mathbb{R}^{N \times 2}$.
% Third, I can leverage the cluster detection algorithm from \emph{bigfish} in order to label each \ac{RNA} node as clustered or not.
% It gives us a boolean $\tilde{X} \in \{0, 1\}^{N \times 1}$ to indicate if a RNA belongs to a RNA cluster of not.
% Depending on whether or not I choose to add the morphological, the clustering or the distance information, I can exploit up to 5 additional dimensions of input.

The original \ac{RNA} point cloud $X \in \mathbb{R}^{N \times 3}$ can be extended in two ways.
First, we can add 2D coordinates from the cell and the nucleus membranes.
As they only exist in 2D, the z-coordinate is chosen arbitrarily to the average height of the \ac{RNA} point cloud (0 if it is centered).
Here, we sampled 300 and 100 points from the cytoplasmic and nuclear membrane, respectively.
Also, we have to make sure that the network can distinguish between the different roles of spatial coordinates of the \ac{RNA}s and the spatial coordinates of nuclear and cytoplasmic membrane.
For this, we add two extra boolean variables for each position (\ac{RNA} and membrane points), indicating their origin: \ac{RNA} points are characterized by $(0,0)$, nuclear membrane points by $(1,0)$ and cytoplasmic membrane by $(0,1)$.
Second, I can compute the distance from cell and nucleus for every \ac{RNA} node.
Third, I can leverage the cluster detection algorithm from \emph{bigfish} in order to label each \ac{RNA} node as clustered or not, represented by an additional boolean variable.
If morphological, distance and clustering information are used, our input matrix is thus $\tilde{X} \in \mathbb{R}^{N \times d}$, where $\tilde{N} = N + 300 + 100$ and $d = 8$ (3D spatial coordinates, 2 dimensions to indicate the spot type, 2 dimensions to indicate distance to nuclear and cytoplasmic membrane and 1 dimension to indicate whether the spot belongs to an \ac{RNA} cluster).

\subsubsection{Model architecture}

I adopt the generic architecture introduced by PointNet~\cite{Qi_2017_CVPR}: successive point-wise representations with increasing depth followed by a max pooling operation to keep the network invariant by input permutation.
I incorporate state-of-the-art modules to learn efficient local structures within the point cloud.
As illustrated in Figure~\ref{fig:PointFISH_architecture}, I also adapt the network to the specificity of RNA point clouds.

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter4/PointFISH_architecture}
    \caption[PointFISH model]{PointFISH architecture, from~\cite{pointfish_2022}.
	Width and height of \textit{boxes} represent output length and dimension, respectively.
	\textit{Tuples} represent output shapes}
    \label{fig:PointFISH_architecture}
\end{figure}

\paragraph{Point-wise Block}

Instead of shared MLPs like PointNet, I implement a multi-head attention layer based on point transformer layer~\cite{Zhao_2021_ICCV}.
First, I assign to each datapoint $x_i$ its 20 nearest neighbors $X(i) \subset X$, based on the euclidean distance in the features space.
I also compute a position encoding $\delta_{ij} = \theta(x_i - x_j)$ for every pair within these neighborhoods, with $\theta$ a MLP.
Three sets of point-wise features are computed for each datapoint, with shared linear projections $\phi$, $\psi$ and $\alpha$.
Relative weights between datapoints $\gamma(\phi(x_i) - \psi(x_j))$ are computed with the subtraction relation (instead of dot product as in the seminal attention paper~\cite{NIPS2017_3f5ee243}) and a MLP $\gamma$.
These attention weights are then normalized by softmax operation $\rho$.
Eventually, datapoint's feature $y_i$ is computed as weighted sum of neighbors value $\alpha(x_j)$, weighted by attention.
With the position encoding added to both the attention weights and the feature value, the entire layer can be summarized as:

\begin{equation}
	{\displaystyle y_i = \sum_{x_j \in X(i)} \rho(\gamma(\phi(x_i) - \psi(x_j) + \delta_{ij})) \odot (\alpha(x_j) + \delta_{ij})}
\end{equation}

For a multi-head attention layer, process is repeated in parallel with independent layers, before a last linear projection merges multi-head outputs.
A shortcut connection and a layer normalization~\cite{ba2016layer} define the final output of my multi-head attention layer.

\paragraph{Alignment Module}

Albeit optional, this module is critical.
Some papers stress the necessity to preprocess the input point cloud by learning a projection to \emph{align} the input coordinates in the right space~\cite{Qi_2017_CVPR,Wang_2019}.
In addition, density heterogeneity across the point cloud and irregular local geometric structures might require local normalization.
To this end, I reuse the geometric affine module described in PointMLP~\cite{ma2022rethinking} which transforms local datapoints to a normal distribution.
With $\{x_{i, j}\}_{j=1,\dots,20} \in \mathbb{R}^{20 \times 3}$, the neighborhood's features of $x_i$, I compute:

\begin{equation}
	{\displaystyle \{x_{i, j}\} = \alpha \odot \frac{\{x_{i, j}\} - x_i}{\sigma + \epsilon} + \beta}
\end{equation}

\noindent
Where $\alpha \in \mathbb{R}^3$ and $\beta \in \mathbb{R}^3$ are learnable parameters, $\sigma$ is the feature deviation across all local neighborhoods and $\epsilon$ is a small number for numerical stability.

\paragraph{Contextual Inputs}

My \ac{RNA} point cloud does not include all the necessary information for a localization pattern classification.
Especially, I missed morphological information.
To this end, deep learning architectures allow flexible insertions.
Several contextual inputs $\tilde{X}$ can feed the network through a parallel branch, before concatenating \ac{RNA} and contextual point-wise features.
My best model exploits cluster and distance information in addition to \ac{RNA} coordinates.

\subsection{Experiment}
\label{subsec:experiment}

\subsubsection{Training and evaluation on simulated patterns}

PointFISH is trained on the simulated dataset.
The implementation is based on TensorFlow~\cite{tensorflow_2015}.
I use Adam optimizer~\cite{Diederik_2015} with a learning rate from 0.001 to 0.00001 and an exponential decay (decay rate of 0.5 every 20,000 steps).
Model is trained for a maximum of 150 epochs, with a batch size of 32, but early stopping criterion is implemented if validation loss does not decrease after 10 consecutive epochs.
Usually model converges after 50 epochs.
I apply a 10\% dropout for the last layer and classifications are evaluated with a categorical cross entropy loss.
Even if localization patterns are not necessarily mutually exclusive, for the simulations I trained the model to predict only one pattern per cell.
For this reason, I did not simulate mixed patterns and assume it could help the model to learn disentangled representations.
Training takes 6 to 8 hours to converge with a Tesla P100 GPU.

A first evaluation can be performed on the simulated test dataset.
Because each pattern is equally generated, a simple accuracy metric is enough.
For the experimental dataset, imbalanced between localization patterns implies a more robust metric like F1-score.
With my best PointFISH models, I obtain a general F1-score of 95\% over the different patterns (see confusion matrix~\ref{fig:confusion_matrix}).

\begin{figure}[]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/chapter4/confusion_matrix}
    \caption[Confusion matrix with simulated test set]{Confusion matrix with simulated test patterns (normalized over the \textit{rows})}
    \label{fig:confusion_matrix}
\end{figure}

\subsubsection{Embedding extraction}

From a trained PointFISH model I can remove the output layer to get a feature extractor that computes a 256-long embedding from a \ac{RNA} point cloud.

\paragraph{Learned embedding}

I compute the embedding for the entire cell population studied in~\cite{CHOUAIB_2020}.
All the 9170 cells can be visualized in 2D using a UMAP projection~\cite{McInnes2018}.
In Figure~\ref{fig:umap_real} every point represents a cell.
Among the 810 annotated cells, those with a unique pattern are colored according to the localization pattern observed in their \ac{RNA} point cloud.
The rest of the dataset remains gray.
Overall, PointFISH embedding discriminates well the different localization patterns.
Intranuclear, nuclear edge and perinuclear cells form distinct clusters, despite their spatial overlap, as well as protrusions.
Cells with foci can be found in a separated cluster, but also mix with nuclear and perinuclear patterns.
This confusion is not surprising as a large number of cells in the dataset present a nuclear-related foci pattern: cells have \ac{RNA}s clustered in foci, which in turn are close to the nuclear envelope, in which case the cell would be labeled with both patterns.

\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter4/umap_real}
    \caption[UMAP embedding with exprimental dataset]{UMAP embedding with learned features, from~\cite{pointfish_2022}.
	Each point is a cell from experimental dataset.
	Manually annotated cells are colored according to their localization pattern}
    \label{fig:umap_real}
\end{figure}

\paragraph{Supervised classification}

Because PointFISH already return meaningful embeddings, I can apply a simple classifier on top of these features to learn localization patterns.
I use the 810 manually annotated cells from the experimental dataset.
I compare the 15 hand-crafted features selected in~\cite{CHOUAIB_2020} with my learned embedding.
Every set of features is rescaled before feeding a classifier.
Expert features include:

\begin{itemize}
	\setlength\itemsep{0.1em}
	\item The number of foci and the proportion of clustered \ac{RNA}s.
	\item The average foci distance from nucleus and cell (normalized by the expected distance with a random foci distribution).
	\item The proportion or \ac{RNA}s inside nucleus.
	\item The average \ac{RNA} distance from nucleus and cell (normalized by the expected distance with a random \ac{RNA} distribution).
	\item The number of \ac{RNA}s detected in cell extensions (normalized by the expected number with a random \ac{RNA} distribution) and the peripheral dispersion index~\cite{stueland_rdi_2019}.
	\item The number of \ac{RNA}s within different relevant subcellular regions (normalized by the expected number with a random \ac{RNA} distribution).
\end{itemize}

\begin{figure}[]
    \centering
	\includegraphics[clip, trim=0cm 0cm 0cm 1cm, width=\textwidth]{figures/chapter4/f1_SVC}
    \caption[F1-score distribution with hand-crafted and learned features]{F1-score distribution of localization pattern classification (SVC model), from~\cite{pointfish_2022}}
    \label{fig:f1_SVC_real}
\end{figure}

I design 5 binary classification tasks, one per localized pattern (random pattern is omitted).
The classifier is a SVC model~\cite{chang2011libsvm}.
For evaluation purpose, I apply a nested cross-validation scheme.
First a test set is sampled from the dataset (20\%), then the remaining cells are used with a gridsearch to fit an optimal SVC model (with a 20\% validation split).
Parameters grid includes the choice between a linear or a RBF kernel and the strength of the regularization.
The entire process is repeated 50 times, with different test split, and F1-score for each classification task is returned.
This full evaluation pipeline is implemented with \emph{scikit-learn}~\cite{scikit-learn}.
F1-score's distribution over 50 splits are summarized in Figure~\ref{fig:f1_SVC_real}.
Learned features match performances of hand-crafted features selected for the tasks.
While the recognition of localization in protrusions is slightly worse, it is important to point out that I did not include simulations of this patterns in the training dataset.

\subsubsection{Ablation study}

I perform several ablation studies to evaluate the impact of different components in PointFISH model.
In the point cloud literature, papers often propose new modules to improve network's performances, but they implement them in slightly different architectures.
Heterogeneity in terms of normalization, latent dimensions or layer size complicate comparisons between techniques.
In order to isolate the importance of each element, I use a template architecture as illustrated in Figure~\ref{fig:PointFISH_architecture}.
Instead of comparing PointFISH with DGCNN, I compare PointFISH with an equivalent network where attention layers are replaced by EdgeConv~\cite{Wang_2019}.
The rest of the network remains strictly identical.

\paragraph{Additional input}

\begin{wraptable}{R}{0.60\textwidth}
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		Distance & Cluster & Morphology & F1-score \\
		\hline
		\ding{55} & \ding{55} & \ding{55} & 0.42\\
		\checkmark & \ding{55} & \ding{55} & 0.74\\
		\ding{55} & \checkmark & \ding{55} & 0.45\\
		\checkmark & \checkmark & \ding{55} & $0.81^{\ast}$\\
		\checkmark & \checkmark & \checkmark & \textbf{0.82}\\
		\hline
	\end{tabular}
	\caption[Impact of contextual inputs (PointFISH)]{Impact of contextual inputs, from~\cite{pointfish_2022}.
	F1-score is averaged over 4 trainings with different random seeds.
	Best model is in bold.
	Reference model is labelled with $\ast$}
	\label{table:extra_inputs}
\end{wraptable}

I compare the use of \ac{RNA} point cloud only as input or the inclusion of contextual inputs through a parallel branch.
\ac{RNA} coordinates do not have any morphological information about the cell.
In Table~\ref{table:extra_inputs}, this design logically returns the lowest F1-score.
Three additional inputs are available: \ac{RNA} distance from cell and nucleus (\emph{distance}), \ac{RNA} clustering flag (\emph{cluster}) and the integration of cell and nucleus membrane coordinates (\emph{morphology}).
Best performances are obtained with, at least, distance and cluster information.
Cell and nucleus coordinates do not increase significantly the classification performance.
The reason is probably that the distance feature already represents the relevant information and adding membrane coordinates does thus not improve performance anymore.
However, inclusion of membrane points dramatically increases the computation time of the model (as a larger point cloud has to be processed).
Cluster information greatly improves foci pattern recognition while distances boost others localization patterns;~both observations are expected.

\paragraph{Alignment module and Point-wise block}

To measure the impact of the geometric affine module~\cite{ma2022rethinking} I compare it with the TNet module implemented in PointNet~\cite{Qi_2017_CVPR}.
I also design a variant TNetEdge where MLP layers extracting point-wise independent features are replaced with EdgeConv layers.
Results are reported in Table~\ref{table:ablation}.
An alignment block seems critical at the beginning of the network.
However, the geometric affine module is both more efficient (F1-score of 0.81) and much lighter than TNet and TNetEdge.

From the PointNet and DGCNN seminal articles, I also compare the use of their respective point-wise blocks against my multi-head attention layer.
As expected, EdgeConv blocks convey a better information than PointNet by exploiting local neighborhood within point cloud (F1-score of 0.78 and 0.75 respectively).
Yet, they do not match the performance of multi-head attention layer.

Concerning these layers, I evaluate how the number of parallel heads can influence the performance of PointFISH.
By default, I use 3 parallel attention layers to let the model specialize its attentions vectors, but I also test 1, 6 and 9 parallel heads.
In Table~\ref{table:ablation} we only observe a slight benefit between the original point transformer layer~\cite{Zhao_2021_ICCV} (without one attention layer) and its augmented implementation.

% multiscale
% k neigbors

\paragraph{Latent dimensions}

The second part of PointFISH architecture is standardized: a first MLP block, a max pooling operation, a second MLP block and the output layer.
I quantify the impact of additional MLP layers within these blocks.
My reference model returns an embedding with 256 dimensions (before the output layer).
In a MLP block, I use ReLU activation and layer normalization, but also increase or decrease the depth by a factor 2 between layers.
Before the pooling layer, the first MLP block includes 4 layers with an increasing depth (128, 256, 512 and 1024).
After the pooling layer, the second MLP block includes 2 layers with a decreasing depth (512 and 256).
Similarly, to return 128, 64 or 32 long embeddings, I implement 6 (128, 256, 512, pooling, 256 and 128), 5 (128, 256, pooling, 128 and 64) or 4 final layers (128, pooling, 64 and 32).
We observe in Table~\ref{table:ablation} a drop in performance for the lowest dimensional embedding (64 and 32).
This hyperparameter is also critical to design lighter models, with a division by 4 in terms of trainable parameters between a 256 and a 128 long embedding.

\begin{table}[]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		Alignment & Point-wise block & \# heads & \# dimensions & \# parameters & F1-score \\
		\hline\hline
		- & Attention layer & 3 & 256 & 1,372,608 & 0.73\\
		TNet & Attention layer & 3 & 256 & 1,712,521 & 0.74\\
		TNetEdge & Attention layer & 3 & 256 & 1,589,321 & 0.74\\
		\hline\hline
		Affine & MLP & - & 256 & 1,374,526 & 0.75\\
		Affine & EdgeConv & - & 256 & 1,387,006 & 0.78\\
		\hline\hline
		Affine & Attention layer & 9 & 256 & 1,403,334 & \textbf{0.82}\\
		Affine & Attention layer & 6 & 256 & 1,387,974 & \textbf{0.82}\\
		Affine & Attention layer & 3 & 256 & 1,372,614 & $0.81^{\ast}$\\
		Affine & Attention layer & 1 & 256 & 1,362,374 & 0.81\\
		\hline\hline
		Affine & Attention layer & 3 & 128 & 352,966 & 0.81\\
		Affine & Attention layer & 3 & 64 & 97,094 & 0.77\\
		Affine & Attention layer & 3 & 32 & 32,646 & 0.75\\
		\hline
	\end{tabular}
	\caption[Ablation studies (PointFISH)]{Ablation studies on experimental dataset, from~\cite{pointfish_2022}.
	F1-score is averaged over 4 trainings with different random seeds.
	Best models are bold.
	Reference model is labelled with $\ast$}
	\label{table:ablation}
\end{table}

\subsection{Discussion}
\label{subsec:discussion}

Being able to directly process list of points provides the community with a tool to integrate large datasets obtained with different techniques on different model systems.
While the actual image data might look strikingly different between such projects, they can all be summarized by segmentation masks of nuclei and cytoplasm, and a list of coordinates of \ac{RNA} locations.
Having methods that act directly on point clouds is therefore a strategic advantage for data integration.

The idea of training on simulated data provides us the opportunity to query datasets with respect to new localization patterns that have not yet been observed, and for which we do not have real examples so far.
In addition, this strategy allows us to control for potential confounders, such as cell morphology, or number of \ac{RNA}s, because we control the composition of the training data and can therefore remove statistical biases a priori.

Finally, I provide a generic method that can leverage these simulations to find a suitable representation, without the tedious process of handcrafting new features.
It is not necessary that the simulated patterns are optimized as to resemble real data: they rather serve as a pretext task.
If a network is capable of distinguishing the simulated patterns, chances are high that the corresponding representation is also informative for slightly or entirely different patterns.
Likewise, representations trained on ImageNet can be used for tumor detection in pathology images.
I show this by omitting the protrusion pattern from the simulation.
Indeed, in Figure~\ref{fig:umap_real}, the protrusion patterns live in a particular region of the feature space, without specific training.
There is however a drop in classification accuracy for the omitted class.
We therefore see that if a class was not specifically simulated, the representation is less optimized with respect to the recognition of that class.
On the other hand, the classification accuracy remains high even for the omitted class.

As a future work, a first valuable task would be the generation of datasets for validation across cell lines and imaging conditions in order to validate the robustness of a point cloud model.
An interesting starting point might be the dataset released in~\cite{savulescu_interrogating_2021, mah_bento_2022}.

A second interesting perspective would be to explicitly address the domain shift between simulated and real data.
Indeed, training a model on simulated data might seem counter-intuitive, as we know that the simulated data follows a different distribution as compared to real data.
Here we argue that the simulations do not necessarily have to reflect the reality in order to lead to powerful simulations;~similar to lessons learned from self-supervised learning, where perturbations also do not need to represent real-world alterations.
Nevertheless it would be interesting to see whether explicit domain adaptation is capable of improving the downstream classification performance.

Another potential improvement concerns the segmentation.
I exploit so far a 2D segmentation because it is the more frequent use-case and the task for which I have a large number of trained models available.
Yet, a 3D segmentation mask would allow a more comprehensive 3D point cloud mixing of \ac{RNA} and cell datapoints.
This could also dramatically improve pattern representation.
Last but not least, training a point cloud model with a self-supervised training procedure would result in task-independent representations and could therefore be an interesting alternative to our simulations. 

\section{Conclusion}
\label{sec:analysis_conclusion}

I first present the coordinate representation of the cell, from which I base my feature engineering and my statistical analysis.
On the top of existing solutions to extract \ac{RNA} spots and cell morphology coordinates, I then list the different hand-crafted features I designed to quantify and classify \ac{RNA} localization patterns.
These features are implemented in \emph{bigfish.classification}.
Lastly, in the third section, I propose to directly process the extracted point clouds, without the need to design handcrafted features.
For this, I leverage coordinates of simulated localization patterns to train a specifically designed neural network taking as input a list of points and associated features that greatly enhance generalization capabilities.
Recent advances in point cloud analysis through deep learning models allows us to build a flexible and scalable pipeline that fits with \ac{RNA} point cloud specificity.
I show that this method is on par with carefully designed, handcrafted feature sets.