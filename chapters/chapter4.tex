%!TEX root = ../main.tex

\graphicspath{{./figures/chapter4/}}


\chapter{Spatial feature engineering} \label{ch:chapter4}
\minitoc
\newpage

In this chapter we present and discuss different approaches to analyze \ac{RNA} localization patterns.
They require to quantify and measure indicators from fluorescent images.
To do so, we use a coordinate representation of cells, merging results from the detection~\ref{ch:chapter2} and segmentation~\ref{ch:chapter3} chapters.

We first present this representation, obtained with methods implemented in \emph{bigfish.multitask}.
In the second and third parts of this chapter we then present two different methods to compute spatial features.
We can manually design localization features or we can learn them, training a gradient-based pipeline on a pretext task.

The hand-crafted features are already implemented in \emph{bigfish.classification}.
The learned features section was mainly developed with the paper:

\begin{center}
	\color{green}
	Future paper to be released (ECCV workshop or arxiv)
\end{center}

\section{From images to coordinates} \label{sec:image_coordinates}

We mainly base our analysis on the coordinate representation of cells as illustrated in figure~\ref{fig:cell_extracted_0}.
We exploit outcomes from detection and segmentation stages.
More precisely, we extract and identify for each individual cell coordinates of detected objects and segmentation masks.
As a reminder, current implementation in \emph{bigfish} allows a 2D or 3D detection, but only a 2D segmentation.
However, any external methods could be used, as long as output format fits.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter4/cell_extracted_0}
    \caption{Original image (left) and coordinate representation (right)}
    \label{fig:cell_extracted_0}
\end{figure}

\subsection{Object labelling} \label{subsec:object_labelling}

In addition to detection and segmentation refinement, the possibility to merge results from both stages is highly valuable.
We can discriminate individual objects according to their localization in the cell.
A user might want to label a detected object if it locates within a segmented surface or not.
For example, some studies require to remove transcription sites before further analysis~\cite{CHOUAIB_2020}, or on the contrary to focus on them.
User could define a \ac{RNA} cluster inside nucleus as a transcription site, as opposed to the ones detected outside nucleus.
More generally, any detected object could be assigned to a specific cellular compartments, depending on the fluorescent labels available in the study.\\

% reference paper using transcription site only

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# discriminate foci and transcription sites
spots_no_ts, foci, ts = multistack.remove_transcription_site(
	rna=spots,
	clusters=clusters,
	nuc_mask=nuc_label,
	ndim=3)
\end{lstlisting}
\end{minipage}

\subsection{Coordinate representation} \label{subsec:coordinate_representation}

\paragraph{Cell extraction}

To extract and summarize our \ac{FISH} results at the single cell level, the only requirement is a segmentation mask of the cell.
At least, user needs to perform instance segmentation to be able to identify individual cells.
Additional results are optional, but they greatly improve the quality of the information assigned to each cell.
Detected \ac{RNA} and cluster (or anything else detected) can be assigned to individual cells.
Nuclei segmentation masks make us able to delimitate nuclear membrane and define transcription sites or any nucleus-related object.
We can also crop input images around the identified cells, for every available channel.

In figure~\ref{fig:cell_extracted_0} we can observe a cropped \ac{smFISH} image on the left, with cell and nuclear membranes in red and blue respectively.
On the right, these membranes are also visible (in black and blue respectively), in addition with \ac{RNA} spots (in red), \ac{RNA} clusters (in orange, with the estimated number of \ac{RNA} clustered) and transcription sites (in green).
With such \emph{extraction} we lose pixel-wise information like intensity values or image texture.
We also deeply rely on detection and segmentation performances to return meaningful coordinates.
Nonetheless, coordinate representation is a sparse and more natural representation for \ac{mRNA} localization pattern classification.
Indeed \ac{mRNA} molecules can be viewed as single point objects distributed in a 3D space.
Microscopic images with fluorescent labels are here the only medium we have to measure and approximate their localization.

Eventually we propose optional criteria to identify individual cells and refine the outcome.
First, we can ensure that only one nucleus is assigned to each cell.
Second, we can remove cropped cells at the border of the \ac{FoV}.
Their segmentation is incomplete and might bias final results.
Third, extracted cells can be filtered out according to the number of detected objects (especially the number of \ac{RNA}).
By censoring empty cells, we remove potential outliers, detection or segmentation failures and therefore help a subsequent statistical analysis.\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# extract cell-level results
fov_results = multistack.extract_cell(
    cell_label=cell_label,
    ndim=3,
    nuc_label=nuc_label,
    rna_coord=spots_no_ts,
    others_coord={"foci": foci, "transcription_site": ts},
    image=image_contrasted,
    others_image={"dapi": nuc_mip, "smfish": smfish_mip})
\end{lstlisting}
\end{minipage}

\paragraph{Statistical description}

At this stage we can already compute standard, but useful statistics for every cell.
We measure cell and nucleus areas, but also \ac{RNA} distribution, inside and outside nucleus.
With cluster coordinates, estimation of cluster size is available, as well as proportion of clustered \ac{RNA}.
The \ac{RNA} proportion in specific cellular compartments are also noteworthy.
Such indicators are already relevant to quantify or validate meaningful biological insights.
For example, a recent study~\cite{cochard_rna_2022} use \emph{bigfish} to estimate \ac{RNA} recruitment in bioengineered condensates (segmented from a \ac{GFP} channel).\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.multistack as multistack

# compute cell-level statistics
df = multistack.summarize_extraction_results(fov_results, ndim=3)
\end{lstlisting}
\end{minipage}

\section{Hand-crafted localization features} \label{sec:hand_features}

We now present the first approach to analyze \ac{RNA} localization patterns in depth: we manually design spatial features.

\subsection{Related work} \label{subsec:related_work_hand_features}

\begin{center}
	\textit{(To be completed)}
\end{center}

% reference classical fish analysis (battich + stoegger + aubin)
% reference dypfish
% reference spatial statistics (carolina whelbi)

%~\cite{savulescu_interrogating_2021}
%~\cite{samacoits_computational_2018}
%~\cite{stoeger_computer_2015}
%~\cite{mcquin_cellprofiler_2018}
%~\cite{held_cellcognition_2010}
%~\cite{berg_ilastik_2019}
%~\cite{shariff_automated_2010}
%~\cite{laux_interactive_2020}
%~\cite{ljosa_introduction_2009}
%~\cite{battich_image-based_2013}
%~\cite{battich_control_2015}

%~\cite{savulescu_dypfish_2019}

%~\cite{ripley2005spatial} spatial statistics

%Feature engineering
%Aubin designed features from the cloud point representation
%of the cell in order to discriminate the different localization patterns. He
%gathered more than 20 features in a tabular format
%(one row per cell), ready to feed a machine learning model.
%A first set of features is based on the euclidean distance. We compute the distance between each
%mRNAs and the centroid of the cell, the centroid of the nucleus, the cell border and
%the nucleus border. Averages and quantiles are then computed from these distances and finally normalized.
%A second set of features involved the Ripley K-function. It quantifies the aggregation or dispersion of mRNAs:
%K(r)=1∑n Npi(r) (7) nλ
%i=1
%with r the distance range, Npi (r) the number of mRNAs in a circle of radius r centered
%on the ith mRNA and λ the total density of mRNAs in the cell (the total number of mRNAs
%normalized by the cell area). Several features are returned from this function like the
%maximum values, the Spearman correlation with the radius r, etc... Such features are
%useful to discriminate foci patterns. A corrected version of the function is actually
%used, less sensitive to boundary effects (mRNAs close to a border have a limited neighbourhood).
%A third set of features is based on morphological opening (an erosion followed by a dilation).
%Applying openings with different sizes, we remove cell’s extensions. We can count the mRNAs
%we loose and compute their proportion with the total number of mRNAs. The idea is to detect
%RNA localization in cell extensions.
%A final group of features concern the proportion of mRNAs inside the nucleus, a dispersion
%and a polarization index.
%1 Ripley: maximum
%2 Ripley: max gradient [0,max]
%3 Ripley: min gradient [max,end]
%4 Ripley: value at mid-point between center and boundary
%5 Ripley: Spearman correlation between Ripley and radius
%6 Ripley: radius of max value
%7 Polarization index
%8 Dispersion index
%9–12 Morph opening–enrichment ratio: 15, 30, 45, 60 pixels
%13 Cell height: Spearman correlation with ZmRNA
%14 Cell height: R2 with ZmRNA
%15 Cell membrane: distance – mean
%16-19 Distance membrane: quantile 5%, 10%, 20%, 50%
%20 Nucleus: distance – mean
%21 Cell centroid: distance – mean
%22 Nucleus centroid: distance – mean
%23 Ratio: mRNAs inside nucleus/outside nucleus

\subsection{Expert features} \label{subsec:expert_features}

A large part of hand-crafted features we implemented in \emph{bigfish.classification} were initially designed for our own studies~\cite{CHOUAIB_2020,safieddine_choreography_2021,pichon_kinesin_2021}.
These features capture more specific information about \ac{RNA} localization, beyond surface areas (\emph{cell\_area} and \emph{nuc\_area}) or expression levels (\emph{nb\_rna}).

\paragraph{Distance features}

We first reuse and adapt some distance features already implemented in the first FISH-quant version and presented in a recent paper~\cite{samacoits_computational_2018}.
Distances from cell or nuclear membranes are computed in 2D as we only use 2D segmentation results so far.
However, such features could be easily extended with 3D segmentation masks.

We compute the average distance between detected \ac{RNA}s and cell membrane \emph{index\_mean\_distance\_cell} such that:

\begin{equation}
	{\displaystyle \operatorname{index\_mean\_distance\_cell} = \frac{\overline{d_{cell}(x_i, y_i)}}{\lambda_{cell}}}
\end{equation}

\noindent
With $d_{cell}(x_i, y_i)$ the euclidean distance to the cell membrane for the rna $i$ and $\lambda_{cell}$ the expected average distance under uniform \ac{RNA} distribution.
A previous study~\cite{battich_control_2015} used the square root of cell area to normalize its distance features.
However, as noticed by~\cite{samacoits_computational_2018}, it does not take into account a potential heterogeneity in terms of cell morphology.
For this reason we approximate $\lambda_{cell}$ as the average value of the 2D distance map from the cell membrane.
Similarly, we compute the normalized average distance of \ac{RNA}s to the nucleus: \emph{index\_mean\_distance\_nuc}.
Alternative computation with the median function is available for these two features too.

Unlike the first FISH-quant version, we do not compute distances to cell or nucleus centroids, nor quantiles of the \ac{RNA} distance distribution.
These features might appear redundant to quantify distance information.

\paragraph{Morphological features}

A second set of features is related to the localization of \ac{RNA} in specific cell compartments.
We already count the number of transcripts detected inside the nucleus.
More precisely, proportion of \ac{RNA}s inside the nucleus (\emph{proportion\_rna\_in\_nuc}) is a good enough indicator to identify intranuclear pattern.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter4/plot_topography}
    \caption{Proportion of \ac{mRNA} at various distances from cell and nuclear membranes}
    \label{fig:features_topography}
\end{figure}

Still we can go further to partition cell regions.
We define a list of features to measure the distribution of \ac{RNA}s within different cell regions.
Regions of interest are delimited with concentric circles from cell or nuclear membranes, with an interval distance of 500nm each.
More precisely, area between 0 and 3000nm from the cell membrane is divided in six concentric regions.
In each region we compute \ac{RNA} proportion (\emph{proportion\_rna\_cell\_radius\_1000\_1500} for the proportion between 1000nm and 1500nm from the cell membrane) or we count the number of \ac{RNA}s we detect, normalized by the expected number of \ac{RNA}s under random distribution (\emph{index\_rna\_cell\_radius\_1000\_1500}).
We define 5 more regions around the nuclear membrane between 500nm and 3000nm.
Lastly, a larger region is defined along the nuclear membrane, including detected \ac{RNA}s inside or outside the nucleus, but within 500nm from its membrane.
Likewise, proportion (\emph{proportion\_rna\_nuc\_radius\_500\_1000}) and \ac{RNA} count (\emph{index\_rna\_nuc\_radius\_500\_1000}) are computed for all regions.
When we manually annotate real cells and identify different localization pattern we measure how discriminative these features can be.
In figure~\ref{fig:features_topography} we can observe average \ac{RNA} proportion for the different regions we described.
The 95\% confidence interval is also reported in the plot.
Logically, nuclear edge and perinuclear patterns present a higher proportion of \ac{RNA} along the nuclear membrane.
On the opposite, cells with a protrusion pattern have a higher \ac{RNA} density along the cell membrane.

We focus on a last relevant region in a cell: protrusions.
To design cell extension related features, we first need to define such extension.
Like~\cite{samacoits_computational_2018} we define this region as the lost area after applying a morphological opening (an erosion followed by a dilation) to the cell mask.
For this operation we use a disk with 3000nm radius as a structuring element.
We then compute the proportion of \ac{RNA} (\emph{proportion\_rna\_protrusion}) or the normalized \ac{RNA} count (\emph{index\_rna\_protrusion}) in protrusion.

\paragraph{Dispersion features}

We implement three features described and tested in a recent paper~\cite{stueland_rdi_2019} to quantify \ac{RNA} polarization and dispersion within the cell.

The polarization index (\emph{index\_polarization}) is computed by comparing \ac{RNA} point cloud and cell centroids:

\begin{equation}
	{\displaystyle \operatorname{index\_polarization} = \frac{\sqrt{(x_{rna} - x_{cell})^2 + (y_{rna} - y_{cell})^2}}{Rg_{cell}}}
\end{equation}

\noindent
With $(x_{rna}, y_{rna})$ the coordinates of the \ac{RNA} centroid and $(x_{cell}, y_{cell})$ the coordinates of the cell centroid.
The radius of gyration $Rg_{cell}$ normalizes the index for different cell sizes.
It is defined as the root-mean-squared distance between every cell pixel and the cell centroid.
The higher, the more polarized \ac{RNA}s are.

The dispersion index (\emph{index\_dispersion}) measures the dispersion of the \ac{RNA} point cloud.
In addition to the extracted coordinates, its computation also implies pixel intensities from the original \ac{smFISH} image:

\begin{equation}
	{\displaystyle \operatorname{index\_dispersion} = \frac{\frac{\sum_{i} r_i^2 I_i}{\sum_{i} I_i}}{\frac{\sum_{j} r_j^2 I_j}{\sum_{j} I_j}}}
\end{equation}

\noindent
With $r_i$ and $r_j$ the euclidean distance of \ac{RNA} $i$ and cell pixel $j$ to the \ac{RNA} centroid respectively, $I_i$ the pixel intensity of \ac{RNA} $i$ and $I_j$ the pixel intensity of cell pixel $j$.
Pixel intensity of transcripts distant from the \ac{RNA} centroid are overweighted.
As the index is normalized considering every pixel $j$ from the cell mask, it tends to 1 when \ac{RNA} point cloud is uniformly distributed.
A diffuse point cloud has a value greater than 1.
On the opposite, if \ac{RNA}s are concentrated anywhere in the cell, index value is lower than 1.

The peripheral distribution index (\emph{index\_peripheral\_distribution}) measures how close the \ac{RNA}s localize to the cell periphery.
Its computation is similar to the dispersion index, but the \ac{RNA} centroid is replaced by the nucleus one in the equation.
A completely dispersed point cloud still has a value of 1, but it increases if \ac{RNA}s move toward the cell periphery, with a concentrated or diffused pattern.
Again, an aggregation of transcripts around the nucleus centroid (often close to the cell centroid too) decreases index value.

\paragraph{Centrosomal features}

\begin{wrapfigure}{R}{0.40\textwidth}
	\begin{center}
	\includegraphics[width=0.33\textwidth]{figures/chapter4/centrosomal_features}
	\caption{Centrosomal \ac{RNA}s (from~\cite{safieddine_choreography_2021})}
	\label{fig:centrosome_features}
	\end{center}
\end{wrapfigure}

If we detect additional object and extract new coordinates besides individual and clustered \ac{RNA}s, more specific features can be designed.
For example, these objects can be cell organelles labelled during the experimentation and targeted for a study.
We design such features with the centrosomes to further study \ac{RNA} localization related to the \ac{MTOC}.

A first obvious feature is the average (or median) distance between \ac{RNA}s and the closest detected centrosomes (up to two centrosomes can be detected in the cell): \emph{index\_mean\_distance\_centrosome}.
Similarly to the distance features we compute with the cell or nuclear membranes, we compute the expected distance under uniform \ac{RNA} distribution for normalization.

A second set of features consists in delimiting an area around each centrome to be considered as centrosome neighbors.
In our case we manually choose a radius of 2000nm around centrosomes to define such areas, as illustrated in figure~\ref{fig:centrosome_features}.
We can then compute the normalized \ac{RNA} count (\emph{index\_rna\_centrosome}) or the \ac{RNA} proportion (\emph{proportion\_rna\_centrosome}) in these regions.

Lastly, we derived a feature from the dispersion index described above.
We define a centrosomal dispersion index to quantify \ac{RNA} dispersion around centrosomes: \emph{index\_centrosome\_dispersion}.
We design it like the dispersion index, but instead of \ac{RNA} centroid, we use the closest centrosome coordinates to compute the euclidean distance.
The lower, the closer \ac{RNA}s localized to the centrosomes.

\paragraph{Cluster features}

Concerning cluster localization patterns, we observed than our clustering method (see subsection~\ref{subsec:dense_decomposition}) already captures good enough information.
More specifically, number of detected clusters (\emph{nb\_foci}) or \ac{RNA} proportion inside these clusters (\emph{proportion\_rna\_in\_foci}) are relevant features to identify transcripts with a tendency for clustering.
As a consequence, and unlike~\cite{samacoits_computational_2018}, we do not implement diverse features based on the famous Ripley's K-function.
This would require tuning a lot more parameters than just reusing the detected number of clusters.

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import bigfish.classification as classification

# compute features
features, features_names = classification.compute_features(
    cell_mask=cell_mask,  # individual cell mask
	nuc_mask=nuc_mask,  # individual nucleus mask
	ndim=3,
	rna_coord=rna_coord,
    smfish=smfish,
	voxel_size_yx=103,  # in nanometer
    foci_coord=foci_coord,
    compute_distance=True,
    compute_intranuclear=True,
    compute_protrusion=True,
    compute_dispersion=True,
    compute_topography=True,
    compute_foci=True,
    compute_area=True,
    return_names=True)
\end{lstlisting}
\end{minipage}

\section{Learned localization features} \label{sec:learned_features}

A second approach to analyze \ac{RNA} localization patterns is to learn relevant features to discriminate transcripts.
To this end we design and train a deep learning model, PointFISH, on a simulated pretext task.
Following transfer learning principles, we reuse the internal representation learned by the model to efficiently discriminate \ac{RNA} localization patterns.

\subsection{Related work} \label{subsec:related_work_learned_features}

Transfer learning a is a frequent techniques in the deep learning literature.
A model train on a first task is reused to build intermediate features for a second and related task, or it is trained again (fine-tuned) on this second task.
We focus on the first meaning of transfer learning, with the goal to build a feature extractor from our model.
Here the first task is just a pretext to pretrain the model and make it learn relevant intermediate representations.

\paragraph{Learning features and embeddings}

Values returned on the last layers of a neural network are a vector representation (or an embedding) of the input.
Well trained, this embedding can be used as numerical features for a subsequent task.
Following this logic, computer vision community progressively replaces hand-crafted features~\cite{Lowe_1999,Bay_2006} by deep learning features to analyze images.
Best convolutional neural networks pretrained on large and general classification challenges are used as backbone or feature extractor for more complex task like face recognition, detection or segmentation.
Common models we can list are ResNet~\cite{He_2016_CVPR}, Inception v3~\cite{Szegedy_2016_CVPR}, EfficientNet~\cite{Tan_2019} or DenseNet~\cite{Huang_2017_CVPR}.
This trend exists in the \ac{NLP} community as well, with the heavy use of word embeddings like Word2vec~\cite{Mikolov_2013}, fastText~\cite{Joulin_2016} or the more recent transformers models.
On graph, a model like node2vec~\cite{Grover_2016} learns ''task-independent representations'' for nodes in networks.

Learning a more general embedding can be a winning strategy when the original task we want to solve has a limited data setting.
Such embedding can be way easier to fit.
An embedding also give us a continuous and numerical representation of a non-structured data like a text or a graph.
With spage2vec~\cite{Partel_2021}, authors learn a low dimensional embedding of local spatial gene expression (they previously expressed as graphs).
They identify meaningful gene expression signatures from tissue datasets.

\paragraph{Convolutional features}

Because we analyze \ac{smFISH} images, a first intuition would be to build a convolutional neural network to directly classify localization patterns from the fluorescent image.
This approach is actually already in place for protein localization.
Unlike \ac{RNA}, proteins can't be resolved individually through \ac{GFP} techniques.
They appear as a gradient of intensity in the cell image and thus protein localization has been approached like texture classification problem.
First studies with microscopy images~\cite{boland_automated_1998} compute a set of feature from the image  (like Zernike moments) before training a classifier.
With recent successes of deep learning, protein localization is now tackled with convolutional neural network, but still framed as a texture classification problem.
Researchers~\cite{sullivan_deep_2018} leveraged crowdsourcing through a video game to collect subcellular localization patterns for the Human Protein Atlas~\cite{Uhlen_2015}.
With these labels they first trained a machine learning model (Loc-CAT) from hand-crafted features,
In a second time, they organized an online challenge~\cite{ouyang_analysis_2019} where a majority of top-ranking solutions were based on convolutional neural networks.
For protein localization the move from hand-crafted features to convolutional features is significant and allows more accurate and robust pipelines.

A recent perspective paper~\cite{Savulescu_2021} foster the use of deep learning models for \ac{RNA} localization analysis.
Today, such analysis can be performed with fluorescent images or \ac{RNA} sequencing.
Authors emphasize the recent successes and flexibility of neural nets with both types of input, and therefore the possibility to design a multimodal pipeline.

We explore the use of convolutional features for \ac{RNA} localization~\cite{dubois_deep_2019}.
However, \ac{smFISH} images have spots and a texture classification approach seems unfit.
Instead of learning from fluorescent images, we build binary images from coordinates representation and train a convolutional neural network with them.
A more detailed description of the method is available in appendix~\ref{ch:convolutional_features}.
Some patterns are not correctly identified and the conversion of coordinates into binary images have some limitations.
We densify a coordinate array in a 2D or 3D image, dramatically increasing the memory needed to process the sample.
With a 2D projected image we also lose relevant spatial information.
Lastly, it has been observed~\cite{Rosanne_2018} that naive convolutional neural networks fail to predict simple coordinates from a coordinate image.
The latter might not be the optimal input format to predict \ac{RNA} localization patterns.

% references CNN for biological images
% Moen, E. et al. Deep learning for cellular image analysis. Nat. Methods https://doi.org/10.1038/s41592-019-0403-1 (2019).
% Godinez, W. J., Hossain, I., Lazic, S. E., Davies, J. W. & Zhang, X. A multi-scale convolutional neural network for phenotyping high-content cellular images. Bioinforma. Oxf. Engl. 33, 2010–2019 (2017).
% Hofmarcher, M., Rumetshofer, E., Clevert, D.-A., Hochreiter, S. & Klambauer, G. accurate prediction of biological assays with high-throughput microscopy images and convolutional networks. J. Chem. Inf. Model. 59, 1163–1171 (2019).
% Kraus, O. Z., Ba, J. L. & Frey, B. J. Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics 32, i52–i59 (2016).

\paragraph{Point cloud models}

We postulate that learning to classify \ac{RNA} localization patterns directly from coordinates is the most efficient approach.
A point cloud has an unordered and irregular structure.
Projecting the coordinates into images or voxels~\cite{Maturana_2015} transforms the problem as an easier vision challenge, but it comes along with some input degradations.

A recent paper~\cite{khater_caveolae_2019} proposes to train a machine learning pipeline to discriminate caveolae clusters from scaffolds clusters.
These cell structures can be recognized through the detection of caveolin-1 proteins and the shape of the resulting point clouds.
Authors compare three pipelines to address the problem: a random forest classifier trained on hand-crafted features, a convolutional neural network applied on the point cloud image and more importantly a PointNet fed directly with point cloud.
Albeit legitimate, the PointNet method is less successful for this task.
Possibly, this model is too naive.
PointNet~\cite{Qi_2017_CVPR} is a seminal work that leads the way for innovative models to address shape classification.
It directly processes point clouds with share MLPs and a max pooling layer, making the network invariant to input permutation.
However, the pooling step is the only way for the model to share information between close points, which ultimately limits its performance.
Yet, recent research dramatically improves point cloud modelling and especially the capture of local information.

PointNet++~\cite{Qi_2017} learns local geometric structures by recursively apply PointNet to different regions of the point cloud, in a hierarchical manner.
This way, local information can be conveyed through the network more efficiently.
DGCNN~\cite{Wang_2019} proposes a new EdgeConv layer where edge features are computed between a point and its neighbors.
Some models propose to adapt convolutions to point cloud by designing new weighting functions or kernel operations like PointCNN~\cite{Li_2018}, PointConv~\cite{Wu_2019_CVPR} or KPConv~\cite{Thomas_2019_ICCV}.
Another inspiration from the computer vision or NLP literature is the attention-based model.
To this end, PointTransformer~\cite{Zhao_2021_ICCV} proposes an attention layer to be applied to local regions within the point cloud.
Last but not least, PointMLP~\cite{ma2022rethinking} proposes a simple but still efficient network with a pure deep hierarchical MLP architecture.

\subsection{Simulated and real tasks} \label{subsec:simulation_real_datasets}

We want to train a model we can directly feed with an input point cloud to obtain a vector representation of this point cloud.
A deep learning model might need a lot of annotated data to reach a convenient performance.
To cope with this requirement, we exploit simulations to train our point cloud model and then use it as a trained feature extractor.
Eventually we use these learned features to classify \ac{RNA} localization patterns on a real dataset.
We define a pretext task to train a point cloud model to generate relevant features in terms of localization patterns.
This pretext task is simply a classification of simulated localization patterns.

\begin{figure}[h]
	\centering
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_10}
		\subcaption{10\% clustered \ac{RNA}}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_50}
		\subcaption{50\% clustered \ac{RNA}}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_foci_90}
		\subcaption{90\% clustered \ac{RNA}}
	\endminipage
	\caption{Foci pattern simulations with increasing pattern strength}
	\label{fig:foci_panel}
\end{figure}

\paragraph{Localization pattern simulations}

To build our simulated dataset, we use methods implemented in \emph{simfish}.
Our package exploits a template of 318 real cells to simulate realistic point clouds.
They were originally extracted for the first version of FISH-quant~\cite{samacoits_computational_2018} to simulate realistic cell and nucleus shapes.
For these template cell, we have their 3D segmentation masks for the cell and the nucleus.
In addition, we have manual annotations about potential protrusions, in case we want to simulate a dedicated pattern.
Several improvements are brought by \emph{simfish}:
\begin{itemize}
	\item We migrate to Python and do not rely on a proprietary framework anymore.
	\item We accelerate the simulation process.
	\item We extend the number of localization patterns available.
	\item We make simulations more consistent in terms of pattern strength.
\end{itemize}

Simulation's outcome includes the cell mask and its membrane coordinates (in 2D), the nucleus mask and its membrane coordinates (in 2D) and the \ac{RNA} coordinates (in 3D).
To match with the rest of the \emph{bigfish} pipeline, we voluntarily return 2D membrane coordinates.
A first parameter to set is the number of \ac{RNA}s $n$ we want to simulate.
To modulate the pattern strength, we set the proportion of \ac{RNA}s $p$ with a biased localization we want.
For example to simulate pattern with a moderate strength, we can simulate between 30\% and 50\% of the \ac{RNA} with the targeted localization bias, and the rest uniformly across the cell.
Lastly, we choose a pattern to simulate.

\begin{figure}[h]
	\centering
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_10}
		\subcaption{10\% perinuclear \ac{RNA}}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_50}
		\subcaption{50\% perinuclear \ac{RNA}}
	\endminipage\hfill
	\minipage{0.33\textwidth}
		\includegraphics[width=\linewidth]{figures/chapter4/simulation_perinuclear_90}
		\subcaption{90\% perinuclear \ac{RNA}}
	\endminipage
	\caption{Perinuclear pattern simulations with increasing pattern strength}
	\label{fig:perinuclear_panel}
\end{figure}

The 9 available patterns are: random, foci, intranuclear, extranuclear, nuclear edge, perinuclear, cell edge, pericellular and protrusion.
Random pattern is the default pattern where \ac{RNA}s are simulated uniformly in the cell.
Foci pattern consists in a random number of \ac{RNA} clusters localizing outside the nucleus.
More precisely, we compute the total number of clustered \ac{RNA}s as $n_{pattern} = n * p$.
We draw the expected number of \ac{RNA}s $\lambda$ per cluster from an uniform distribution between 5 and 21 \ac{RNA}s.
For each cluster, a distinct number of \ac{RNA}s is draw again with a Poisson distribution of mean $\lambda$.
Clusters are then localized outside the nucleus and remaining \ac{RNA}s uniformly in the cell.
In figure~\ref{fig:foci_panel} we can observe foci simulations with an increasing pattern strength (from 10\% to 90\% of clustered \ac{RNA}s)

Others patterns are simulated with a common scheme.
In a first step we generate a probability map to bias the localization of $n_{pattern}$ \ac{RNA}s.
In a second step we complete the point cloud with random \ac{RNA}s until we reach the expected number of transcripts.
Intranuclear pattern has an uniform probability map inside the nucleus and zeros outside.
Extranuclear pattern is the exact opposite, with nonzero probabilities outside the nucleus and zeros inside.
Nuclear edge and cell edge have nonzero probabilities along the nuclear and cell membranes.
Similarly, perinuclear and pericellular are patterns where \ac{RNA}s are polarized towards nuclear and cell membranes.
Perinuclear probability map is build from the cell distance map.
We contrast the euclidean distance by computing its quadratic values.
The same operation is performed to build the pericellular probability map, using the nucleus distance map.
As a result, for pericellular pattern, \ac{RNA}s have a higher probability to localize in regions distant from nucleus.
Protrusion pattern has a uniform probability map within the annotated protrusion regions and zeros everywhere else.
In figure~\ref{fig:perinuclear_panel} different perinuclear simulations can be observed as an example.
In addition, an overview of every simulated pattern is available in appendix~\ref{sec:appendix_simulations_pattern}.\\

\begin{minipage}{0.9\textwidth}
\begin{lstlisting}[language=Python]
import simfish as sim

# load template dataset
path_template_directory = load_extract_template(path_output)

# localization pattern simulation
instance_coord = sim.simulate_localization_pattern(
	path_template_directory,
	n_spots=150,
	pattern="intranuclear",
	proportion_pattern=0.6)
\end{lstlisting}
\end{minipage}

\paragraph{Simulated dataset}

With \emph{simfish} we simulate a dataset with 8 different localization patterns: random, foci, intranuclear, extranuclear, nuclear edge, perinuclear, cell edge and pericellular.
We choose these patterns to equally cover the entire cell regions.
We keep protrusion simulations apart to prevent the model from training on a too specific localization pattern.
We also want to see if our pipeline can extrapolate relevant embedding for unknown patterns.
As we have protrusions in the real dataset, we can evaluate such extrapolation.
We simulate 20,000 samples for each pattern and between 50 to 900 \ac{RNA}s per cell, so a full dataset of 160,000 simulated cells.
Random and protrusion patterns excepted, every other simulation have a proportion of \ac{RNA}s with a biased localization spanning from 60\% to 100\%.
In protrusion pattern only 20 to 50\% of \ac{RNA}s are localizing in cell extensions.
With random simulations the pattern strength has no effect.
We split our dataset between train, validation and test, with 60\%, 20\% and 20\% respectively.
We make sure that simulations from the same cell template can't be assigned to different splits.
Finally, point clouds are augmented with random rotation along the up-axis, centered and normalized into the unit sphere.

\paragraph{Real dataset}

We use the real dataset extracted from our study~\cite{CHOUAIB_2020}.
After cleaning, it consists in 9710 individual cells, with images cropped and coordinates extracted.
Cells have 346 \ac{RNA}s in average and 90\% of them have between 39 and 1307 transcripts.
This dataset is extracted from a \ac{smFISH} study on HeLa cell line and targets 27 genes.
Furthermore, 810 cells have been manually annotated with localization patterns by biologists.
These patterns are not mutually exclusive.
Distribution of these annotates patterns is detailed in table~\ref{table:real_dataset}.

\begin{wraptable}{L}{0.40\textwidth}
	\centering
	\begin{tabular}{| c | c |}
		\hline
		Pattern & \# of cells \\
		\hline
		Random & 372\\
		Foci & 198\\
		Intranuclear & 73\\
		Nuclear edge & 87\\
		Perinuclear & 64\\
		Protrusion & 83\\
		\hline
	\end{tabular}
	\caption{Annotated real dataset}
	\label{table:real_dataset}
\end{wraptable}

% add FISH image as example
% add real point cloud as example

\subsection{PointFISH} \label{subsec:pointfish}

PointFISH exploits several modules developed in the literature on deep learning models for point cloud analysis and more precisely for shape classification.
We implement a parallel branch to integrate information about cell morphology, with membrane coordinates directly, precomputed distances or cluster detection results.
Trained on a large simulated dataset, PointFISH allows us compute an embedding for any \ac{RNA} point cloud.

\subsubsection{Input preparation}

Besides the original \ac{RNA} point cloud, we can use an optional second input vector with our model.
Let's $X \in \mathbb{R}^{N \times 3}$ be the original input point cloud with $N$ the number of \ac{RNA}s.
We define our second input vector as $\tilde{X} \in \mathbb{R}^{N \times d}$ with $d \in \{1, 2, 3, 4, 5\}$.
The latter is composed of three optional inputs to enrich our model.
First we can integrate morphological information by merging \ac{RNA} point cloud with 2D coordinates from the cell and the nucleus membranes.
Such coordinates are localized to the average height of the \ac{RNA} point cloud (0 if it is centered).
This morphological input substantially increases the size of the original point cloud, because we subsample 300 nodes from the cell membrane and 100 nodes from the nucleus one.
We also define an extra boolean vector to indicate the cell nodes and a second one to label the nucleus nodes.
By construction, the original \ac{RNA} point cloud corresponds to two \emph{False} values.
We end up with $X \in \mathbb{R}^{\tilde{N} \times 3}$ (with $\tilde{N} = N + 300 + 100$) and $\tilde{X} \in \{0, 1\}^{\tilde{N} \times 2}$ as inputs.
Second, we can compute the distance from cell and nucleus for every \ac{RNA} node.
This adds an extra input $\tilde{X} \in \mathbb{R}^{N \times 2}$.
Third, we can leverage the cluster detection algorithm from \emph{bigfish} in order to label each \ac{RNA} node as clustered or not.
It gives us a boolean $\tilde{X} \in \{0, 1\}^{N \times 1}$ as extra input.
Depending on whether or not we choose to add the morphological, the clustering or the distance information, we can exploit up to 5 extra dimensions of input.

\subsubsection{Model architecture}

\begin{center}
	\textit{(To be completed)}
\end{center}

\paragraph{Point-wise Block}

\paragraph{Alignment Module}

\paragraph{Contextual Inputs}

% architecture description
% plot architecture

%- why template ?

%- point-wise block (attention layer, mlp, edgeconv)
%- pooling

%(optional)
%- align block (affine, tnet, tnetedge)
%- MLP block before
%- MLP block after
%- Extra features before pooling

%- best model used

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter4/PointFISH_architecture}
    \caption{PointFISH architecture template}
    \label{fig:PointFISH_architecture}
\end{figure}

\subsection{Experiment} \label{subsec:experiment}

We did not design PointFISH to classify localization patterns, but to learn continuous representations first.
Therefore, we train it on simulated dataset, with realistic but different localization patterns observed in real cells.
Afterwards, we reuse its internal representations for unsupervised and supervised analysis with a real dataset.

\subsubsection{Training and evaluation on simulated patterns}

We train PointFISH on the simulated dataset.
Our implementation is based on TensorFlow~\cite{tensorflow_2015}.
We use Adam optimizer~\cite{Diederik_2015} with a learning rate from 0.001 to 0.00001 and an exponential decay (decay rate of 0.5 every 20,000 steps).
Model is trained for a maximum of 150 epochs, with a batch size of 32, but early stopping criterion is implemented if validation loss does not decrease after 10 consecutive epochs.
Usually model converges after 50 epochs.
We apply a 10\% dropout for the last layer and classifications are evaluated with a categorical cross entropy loss.
Even if localization patterns are not necessarily exclusive, for the simulations we trained the model to predict only one pattern per cell.
For this reason, we did not simulate mixed patterns and assume it could help the model to learn disentangled representations.
Training takes 6 to 8 hours to converge with a Tesla P100 GPU.

A first evaluation can be performed on the simulated test dataset.
Because each pattern is equally generated, a simple accuracy metric is enough.
On real dataset, imbalanced between localization patterns implies a more robust metric like F1-score.
With our best PointFISH models, we obtain a general accuracy of 95\% over the different patterns (see confusion matrix~\ref{fig:confusion_matrix}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/chapter4/confusion_matrix}
    \caption{Confusion matrix with simulated patterns}
    \label{fig:confusion_matrix}
\end{figure}

\subsubsection{Embedding extraction}

From a trained PointFISH model we can remove the output layer to get a feature extractor that computes a 256-long embedding from a \ac{RNA} point cloud.

\paragraph{Learned embedding}

We compute the embedding for the entire cell population studied in~\cite{CHOUAIB_2020}.
All the 9170 cells can be visualized in 2D using a UMAP projection~\cite{McInnes2018}.
In figure~\ref{fig:umap_real} every point represents a cell.
Among the 810 annotated cells, those with a unique pattern are colored according to the localization pattern observed in their \ac{RNA} point cloud.
The rest of the dataset remains gray.
Overall, PointFISH embedding discriminates well the different localization patterns.
Intranuclear, nuclear edge and perinuclear cells form distinct clusters, despite their spatial overlap, as well as protrusions.
Foci cells gather in a separated clusters as well, but also mix with nuclear and perinuclear patterns.
This confusion is not surprising as a large number of cells in the dataset present a nuclear-related foci pattern.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/chapter4/umap_real}
    \caption{UMAP embedding with learned features}
    \label{fig:umap_real}
\end{figure}

\paragraph{Supervised classification}

Because PointFISH already returns meaningful embeddings, we can apply a simple classifier on top of these features to learn localization patterns.
We use the 810 manually annotated cells from the real dataset.
We compare the hand-crafted features selected in~\cite{CHOUAIB_2020} with our learned embedding.
Every set of features is rescaled before feeding a classifier.
The former includes 15 features:

\begin{itemize}
	\item The number of foci and the proportion of clustered \ac{RNA}.
	\item The average foci distance from nucleus and cell (normalized by the expected distance with a random foci distribution).
	\item The proportion or \ac{RNA} inside nucleus.
	\item The average \ac{RNA} distance from nucleus and cell (normalized by the expected distance with a random \ac{RNA} distribution).
	\item The number of \ac{RNA}s detected in cell extensions (normalized by the expected number with a random \ac{RNA} distribution) and the peripheral dispersion index~\cite{stueland_rdi_2019}.
	\item The number of \ac{RNA}s within different relevant subcellular regions (normalized by the expected number with a random \ac{RNA} distribution).
\end{itemize}

\begin{figure}[h]
    \centering
	\includegraphics[clip, trim=0cm 0cm 0cm 1cm, width=1\textwidth]{figures/chapter4/f1_SVC}
    \caption{F1-score with localization pattern classification (SVC model)}
    \label{fig:f1_SVC_real}
\end{figure}

We design 5 binary classification tasks, one per localized pattern (random pattern is omitted).
The classifier is a SVC model~\cite{Platt99probabilisticoutputs,chang2011libsvm}.
For evaluation purpose, we apply a nested cross-validation scheme.
First a test dataset is sampled the dataset (20\%), then the remaining cells are used with a gridsearch to fit an optimal SVC model (with a 20\% validation split).
Parameters grid includes the choice between a linear or a RBF kernel and the strength of the regularization.
The entire process is repeated 50 times, with different test split, and F1-score for each classification task is returned.
This full evaluation pipeline is implemented with \emph{scikit-learn}~\cite{scikit-learn}.
F1-score's distribution over 50 splits are summarized in figure~\ref{fig:f1_SVC_real}.
Learned features match performances of hand-crafted features selected for the tasks.
Only protrusion recognition is slightly worse, but it is also the only pattern that is not simulated.

\subsubsection{Ablation study}

We perform several ablation studies to evaluate the impact of different components in PointFISH model.
In the point cloud literature, papers often propose new modules to improve network's performances, but they implement them in slightly different architectures.
Heterogeneity in terms of normalization, latent dimensions or layer size complicate comparisons between techniques.
In order to isolate the importance of each element, we use a template architecture as illustrated in figure~\ref{fig:PointFISH_architecture}.
Instead of comparing PointFISH with DGCNN, we compare PointFISH with an equivalent networks where attention layers are replaced by EdgeConv~\cite{Wang_2019}.
The rest of the network remains strictly identical.

\paragraph{Additional input}

\begin{wraptable}{R}{0.60\textwidth}
	\centering
	\begin{tabular}{| c | c | c | c |}
		\hline
		Distance & Cluster & Morphology & F1-score \\
		\hline
		\ding{55} & \ding{55} & \ding{55} & 0.42\\
		\checkmark & \ding{55} & \ding{55} & 0.74\\
		\ding{55} & \checkmark & \ding{55} & 0.45\\
		\checkmark & \checkmark & \ding{55} & $0.81^{\ast}$\\
		\checkmark & \checkmark & \checkmark & \textbf{0.82}\\
		\hline
	\end{tabular}
	\caption{Impact of contextual inputs.
	F1-score is averaged over 4 trainings with different random seeds.
	Best model is in bold.
	Reference model is labelled with $\ast$}
	\label{table:extra_inputs}
\end{wraptable}

We compare the use of \ac{RNA} point cloud only as input or the inclusion of secondary inputs through a parallel branch.
\ac{RNA} coordinates do not have any morphological information about the cell.
In table~\ref{table:extra_inputs}, this design logically returns the lowest F1-score.
Three additional inputs are available: \ac{RNA} distance from cell and nucleus (\emph{distance}), \ac{RNA} clustering flag (\emph{cluster}) and the integration of cell and nucleus membrane coordinates (\emph{morphology}).
Best performances are reached with at least distance and cluster information.
Cell and nucleus coordinates do not increase significantly the classification and dramatically increase the computation time of the model (we need to process a larger point cloud).
In particular, cluster information greatly improves foci pattern recognition while distances boost others localization patterns.

\paragraph{Alignment module and Point-wise block}

To measure the impact of the geometric affine module~\cite{ma2022rethinking} we compare it with the TNet module implemented in PointNet~\cite{Qi_2017_CVPR}.
We also design a variant TNetEdge where MLP layers extracting point-wise independent features are replaced with EdgeConv layers.
Results are reported in table~\ref{table:ablation}.
An alignment block seems critical at the beginning of the network.
However, the geometric affine module is both more efficient (F1-score of 0.81) and much lighter than TNet and TNetEdge.

From PointNet and DGCNN seminal papers we also compare the use of their respective point-wise blocks compare to our multi-attention layers.
As expected, EdgeConv blocks convey a better information than PointNet by exploiting local neighborhood within point cloud (F1-score of 0.78 and 0.75 respectively).
Yet, they do not match the performance of multi-attention layers.

Concerning these layers, we evaluate how the number of parallel heads can influence the performance of PointFISH.
By default, we use 3 parallel attention layers to let the model specialized its attentions vectors, but we also test 1, 6 and 9 parallel heads.
In table~\ref{table:ablation} we only observe a slight benefit between the original point transformer layer~\cite{Zhao_2021_ICCV} (without one attention layer) and its augmented implementation.

% multiscale

\paragraph{Latent dimensions}

The second part of PointFISH architecture is standardized: a first MLP block, a max pooling operation, a second MLP block and the output layer.
We quantify the impact of additional MLP layers within these blocks.
Our reference model returns an embedding with 256 dimensions (before the output layer).
In a MLP block, we increase or decrease the depth by a factor 2 between layers.
Before the pooling layer, the first MLP block includes 4 layers with an increasing depth (128, 256, 512 and 1024).
After the pooling layer, the second MLP block includes 2 layers with a decreasing depth (512 and 256).
Similarly, to return 128, 64 or 32 long embeddings, we implement 6 (128, 256, 512, pooling, 256 and 128), 5 (128, 256, pooling, 128 and 64) or 4 final layers (128, pooling, 64 and 32).
We observe in table~\ref{table:ablation} a fall in performance for the lowest dimensional embedding (64 and 32).
This hyperparameter is also critical to design lighter models, with a division by 4 in terms of trainable parameters between a 256 and a 128 long embedding.

\begin{table}[h]
	\centering
	\begin{tabular}{| c | c | c | c | c | c |}
		\hline
		Alignment & Point-wise block & \# heads & \# dimensions & \# parameters & F1-score \\
		\hline\hline
		- & Attention layer & 3 & 256 & 1,372,608 & 0.73\\
		TNet & Attention layer & 3 & 256 & 1,712,521 & 0.74\\
		TNetEdge & Attention layer & 3 & 256 & 1,589,321 & 0.74\\
		\hline\hline
		Affine & MLP & - & 256 & 1,374,526 & 0.75\\
		Affine & EdgeConv & - & 256 & 1,387,006 & 0.78\\
		\hline\hline
		Affine & Attention layer & 9 & 256 & 1,403,334 & \textbf{0.82}\\
		Affine & Attention layer & 6 & 256 & 1,387,974 & \textbf{0.82}\\
		Affine & Attention layer & 3 & 256 & 1,372,614 & $0.81^{\ast}$\\
		Affine & Attention layer & 1 & 256 & 1,362,374 & 0.81\\
		\hline\hline
		Affine & Attention layer & 3 & 128 & 352,966 & 0.81\\
		Affine & Attention layer & 3 & 64 & 97,094 & 0.77\\
		Affine & Attention layer & 3 & 32 & 32,646 & 0.75\\
		\hline
	\end{tabular}
	\caption{Ablation studies on real dataset~\cite{CHOUAIB_2020}.
	F1-score is averaged over 4 trainings with different random seeds.
	Best models are bold.
	Reference model is labelled with $\ast$}
	\label{table:ablation}
\end{table}

\section{Conclusion} \label{sec:analysis_conclusion}

\begin{center}
	\textit{(To be completed)}
\end{center}

% learned features from convnet
% from features engineering to simulation engineering
% limitations